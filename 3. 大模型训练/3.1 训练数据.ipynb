{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 数据源\n",
    "大模型训练数据集的构建直接决定了模型的性能上限，以下是当前最具代表性的数据集，具备 **规模大、质量高、多样性好** 的特点。\n",
    "\n",
    "### **通用预训练数据集**\n",
    "#### **（1）[Common Crawl](https://commoncrawl.org/)**  \n",
    "- **类型**：网页文本  \n",
    "- **规模**：原始数据约 250TB（净文本约 3TB）  \n",
    "- **特点**：  \n",
    "  - 覆盖全球数十亿网页，包含多语言内容  \n",
    "  - 需严格清洗（去重、过滤低质量/有害内容）  \n",
    "- **示例**：GPT-3（60%数据来自Common Crawl）、PaLM、LLaMA 等模型的核心数据源\n",
    "\n",
    "#### **（2）[The Pile](https://pile.eleuther.ai/)**  \n",
    "- **类型**：多样化文本集合，  \n",
    "- **规模**：825GB（22个子集）\n",
    "\n",
    " <a href=\"https://arxiv.org/pdf/2101.00027\" target=\"_blank\">\n",
    "  <center>\n",
    "  <img src='./images/the_pile.png' alt=\"The Pile\" height='600' >\n",
    "  </center>\n",
    "</a>\n",
    "\n",
    "- **特点**：  \n",
    "  - 包含学术论文（arXiv）、书籍（Bibliotik）、代码（GitHub）等专业领域数据  \n",
    "  - 提供清晰的数据来源和清洗日志  \n",
    "- **示例**：PubMed（医学论文）、StackExchange（技术问答）；专为开源社区设计，支撑Pythia、GPT-NeoX等模型\n",
    "\n",
    "#### **（3）[C4 (Colossal Cleaned Common Crawl)](https://huggingface.co/datasets/c4)**  \n",
    "- **类型**：清洗后的网页文本  \n",
    "- **规模**：净文本 750GB  \n",
    "- **特点**：  \n",
    "  - 基于Common Crawl进行自动化过滤（保留英文、去除重复/低质量内容）  \n",
    "  - 标准化处理流程（Heuristic规则+语言模型筛选）  \n",
    "- **示例**：T5、BERT后续版本的标准训练集  \n",
    "\n",
    "### **领域专用数据集**\n",
    "#### **（1）[BooksCorpus](http://yknzhu.wixsite.com/mbweb)**  \n",
    "- **类型**：小说类书籍文本  \n",
    "- **规模**：约10GB（11,038本书）  \n",
    "- **特点**：  \n",
    "  - 长文本连贯性强，适合训练生成能力  \n",
    "  - 版权受限（未公开分发）  \n",
    "- **示例**：GPT-1/GPT-2的核心数据源，推动自回归模型发展  \n",
    "\n",
    "#### **（2）arXiv Dataset**  \n",
    "- **类型**：学术论文（LaTeX源码）  \n",
    "- **规模**：2.5TB（含数学公式和图表）  \n",
    "- **特点**：  \n",
    "  - 包含STEM领域前沿知识  \n",
    "  - 需处理公式/引用等特殊结构  \n",
    "- **示例**：Galactica、Minerva等科学领域模型的训练基础  \n",
    "\n",
    "#### **（3）CodeParrot**  \n",
    "- **类型**：代码数据（GitHub公开仓库）  \n",
    "- **规模**：1TB（Python/Java等代码）  \n",
    "- **特点**：  \n",
    "  - 包含代码注释与实现，支持代码生成任务  \n",
    "  - 过滤低质量代码（Star数/License合规性）  \n",
    "- **示例**：Codex、StarCoder等代码模型的训练基础  \n",
    "\n",
    "### **多模态数据集**\n",
    "#### **（1）LAION-5B**  \n",
    "- **类型**：图文对（Image-Text Pair）  \n",
    "- **规模**：58.5亿对（图像+描述文本）  \n",
    "- **特点**：  \n",
    "  - 从Common Crawl提取的网页图文数据  \n",
    "  - 提供CLIP过滤后的高质量子集（LAION-400M）  \n",
    "- **示例**：Stable Diffusion、DALL·E 2等文生图模型的训练核心  \n",
    "\n",
    "#### **（2）WebVid**  \n",
    "- **类型**：视频-文本对  \n",
    "- **规模**：1000万视频片段（总时长约50,000小时）  \n",
    "- **特点**：  \n",
    "  - 视频片段与描述文本对齐  \n",
    "  - 包含多样化场景（自然/人文/科技等）  \n",
    "- **示例**：VideoGPT、NÜWA等多模态视频生成模型的基础  \n",
    "\n",
    "### **指令微调数据集**\n",
    "#### **（1）FLAN Collection**  \n",
    "- **类型**：任务指令数据集  \n",
    "- **规模**：1,836个NLP任务（分类/生成/推理等）  \n",
    "- **特点**：  \n",
    "  - 将传统NLP任务转化为指令格式  \n",
    "  - 支持跨任务泛化能力  \n",
    "- **示例**：Flan-T5、Flan-PaLM等指令微调模型的核心  \n",
    "\n",
    "#### **（2）Alpaca-52K**  \n",
    "- **类型**：人工标注指令数据  \n",
    "- **规模**：52,000条（基于Self-Instruct生成）  \n",
    "- **特点**：  \n",
    "  - 模仿ChatGPT的指令-输出对  \n",
    "  - 低成本生成高质量微调数据  \n",
    "- **示例**：开源社区微调LLaMA/Alpaca的核心数据集  \n",
    "\n",
    "### **中文数据集**\n",
    "#### **（1）WuDaoCorpora（悟道）**  \n",
    "- **类型**：多源中文语料  \n",
    "- **规模**：5TB（清洗后净文本）  \n",
    "- **特点**：  \n",
    "  - 包含百科、新闻、书籍、论坛等来源  \n",
    "  - 采用严格去重和敏感信息过滤  \n",
    "- **示例**：GLM-130B、ChatGLM的预训练基础  \n",
    "\n",
    "#### **（2）CLUECorpus2020**  \n",
    "- **类型**：中文通用语料  \n",
    "- **规模**：100GB（包含文本分类/摘要等任务）  \n",
    "- **特点**：  \n",
    "  - 细分法律、医疗、金融等垂直领域  \n",
    "  - 提供预处理版本（分词/去噪）  \n",
    "- **示例**：ERNIE、PanGu-α等中文模型的基准数据集  \n",
    "\n",
    "### **数据集比较**\n",
    "| 数据集         | 规模优势          | 领域覆盖      | 开源状态       | 代表性模型        |\n",
    "|----------------|-------------------|---------------|----------------|-------------------|\n",
    "| Common Crawl   | ★★★★★ (250TB+)   | 通用/多语言   | 公开           | GPT-3、LLaMA     |\n",
    "| The Pile       | ★★★★ (825GB)      | 多领域        | 受限访问       | GPT-NeoX、Pythia |\n",
    "| LAION-5B       | ★★★★★ (5B图文对) | 多模态        | 公开           | Stable Diffusion |\n",
    "| WuDaoCorpora   | ★★★★☆ (5TB)       | 中文专用      | 非公开         | GLM-130B         |\n",
    "| FLAN Collection| ★★★ (1.8K任务)    | 任务指令      | 公开           | Flan-T5          |\n",
    "\n",
    "\n",
    "### **数据选择建议**\n",
    "1. **预训练**：优先选择 **Common Crawl/The Pile** + **领域数据（如arXiv/CodeParrot）** 的组合  \n",
    "2. **指令微调**：使用 **FLAN Collection** 提升泛化能力，结合 **Alpaca-52K** 优化对话表现  \n",
    "3. **中文模型**：以 **WuDaoCorpora** 为主力，辅以 **CLUECorpus2020** 增强领域适应性  \n",
    "\n",
    "数据集的构建需平衡规模与质量，建议参考Meta的《The RefinedWeb Dataset》提出的数据过滤方法（基于分类器+规则），可减少低质量数据对模型性能的影响。\n",
    "\n",
    "**【数据下载】**：\n",
    "[魔搭社区数据集](https://modelscope.cn/datasets)\n",
    "[LLMDataHub](https://github.com/Zjh-819/LLMDataHub)\n",
    "[OpenDataLab](https://opendatalab.com/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 数据处理 \n",
    "\n",
    "预训练数据通常需经过以下处理：\n",
    "1. **清洗**：去重、过滤低质量文本、标准化编码。\n",
    "2. **分块（Chunking）**：将长文本切分为固定长度的块（如1024 tokens）。\n",
    "3. **分词**：转换为Token ID序列（如使用BPE、WordPiece等）。\n",
    "4. **添加特殊标记**：`[CLS]`、`[SEP]`、`<|endoftext|>`等。\n",
    "\n",
    "### 预训练数据格式 \n",
    "大模型预训练数据集的格式通常需要满足高效存储、快速读取和灵活处理的需求。以下是几种常用格式及其特点：\n",
    "#### **纯文本格式（Text Files）**\n",
    "- **格式说明**：最简单的格式，每个文档或段落以换行符分隔。\n",
    "- **示例**：\n",
    "  ```text\n",
    "  This is the first document.\n",
    "  This is the second document...\n",
    "  ```\n",
    "- **优点**：\n",
    "  - 轻量级，易于生成和调试。\n",
    "  - 兼容所有框架（如Hugging Face、Megatron-LM等）。\n",
    "- **缺点**：\n",
    "  - 缺乏元数据（如文档来源、类别）。\n",
    "  - 大规模数据时需分片存储（如按`part-00001.txt`分片）。\n",
    "\n",
    "#### **JSON/JSON Lines（JSONL）**\n",
    "- **格式说明**：\n",
    "  - **JSON**：整个数据集存储为一个大JSON数组，每个元素为一个文档。\n",
    "  - **JSON Lines**：每行一个独立的JSON对象（按行分隔）。\n",
    "- **示例**（JSONL）：\n",
    "  ```json\n",
    "  {\"text\": \"This is document 1\", \"source\": \"wiki\", \"id\": 1}\n",
    "  {\"text\": \"This is document 2\", \"source\": \"book\", \"id\": 2}\n",
    "  ```\n",
    "- **优点**：\n",
    "  - 支持元数据（如文档来源、时间戳）。\n",
    "  - JSONL适合流式读取（无需加载整个文件）。\n",
    "- **缺点**：\n",
    "  - 解析速度较纯文本慢（需处理JSON结构）。\n",
    "  - 文件体积较大（可通过压缩优化）。\n",
    "\n",
    "#### **TFRecord（TensorFlow格式）**\n",
    "- **格式说明**：TensorFlow专用的二进制格式，支持分片存储和高效读取。\n",
    "- **特点**：\n",
    "  - 数据需序列化为`tf.train.Example` Protobuf对象。\n",
    "  - 支持并行读取和分布式训练。\n",
    "- **优点**：\n",
    "  - 高性能IO（适合超大规模数据集）。\n",
    "  - 与TensorFlow生态无缝集成。\n",
    "- **缺点**：\n",
    "  - 需要预先定义数据模式（Schema）。\n",
    "  - 非TensorFlow框架需额外解析。\n",
    "\n",
    "#### **HDF5**\n",
    "- **格式说明**：科学计算中常用的二进制格式，支持多维数组存储。\n",
    "- **适用场景**：\n",
    "  - 存储预处理后的分词结果（如Token ID数组）。\n",
    "  - 适合固定长度的数据块（如512 tokens/block）。\n",
    "- **优点**：\n",
    "  - 高压缩率，快速读取。\n",
    "  - 支持随机访问（无需顺序读取）。\n",
    "- **缺点**：\n",
    "  - 灵活性较低（修改数据需重建文件）。\n",
    "\n",
    "#### **内存映射格式（Memory-Mapped Formats）**\n",
    "- **格式说明**：将数据文件直接映射到内存，减少IO开销。\n",
    "- **常见实现**：\n",
    "  - Hugging Face `datasets`库的`mmap`模式。\n",
    "  - PyTorch的`np.memmap`。\n",
    "- **优点**：\n",
    "  - 适合单机多GPU训练（快速随机访问）。\n",
    "  - 支持超大规模文件（如TB级）。\n",
    "- **缺点**：\n",
    "  - 需要预处理为固定大小的数据块。\n",
    "\n",
    "#### **自定义二进制格式**\n",
    "- **格式说明**：针对特定框架设计的二进制格式（如Megatron-LM的`BERT数据集`）。\n",
    "- **特点**：\n",
    "  - 数据预分词并存储为Token ID数组。\n",
    "  - 索引文件记录每个样本的偏移量。\n",
    "- **优点**：\n",
    "  - 极致性能（零解析开销）。\n",
    "  - 支持动态掩码（Dynamic Masking）。\n",
    "- **缺点**：\n",
    "  - 需与训练代码深度绑定，灵活性差。\n",
    "\n",
    "### **建议**\n",
    "- **小规模实验**：使用JSONL或纯文本，便于调试。\n",
    "- **超大规模训练**：优先选择TFRecord、HDF5或内存映射格式。\n",
    "- **跨框架兼容**：推荐Parquet或压缩后的JSONL（如`jsonl.gz`）。\n",
    "\n",
    "下面的程序完成是一个word文档的数据预处理，包括格式化、去重、同义词替换等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from docx import Document\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "print(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "synonym_dict = defaultdict(list)\n",
    "with open(\"synonyms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "            synonym_dict[word].extend([w for w in words if w != word])\n",
    "\n",
    "class DocProcessor:\n",
    "    def __init__(self):\n",
    "        self.seen_hashes = set()\n",
    "        self.synonym_cache = {}\n",
    "        \n",
    "    def read_docx(self, file_path):\n",
    "        doc = Document(file_path)\n",
    "        full_text = []\n",
    "        \n",
    "        for para in doc.paragraphs:\n",
    "            if not self._is_header_footer(para):\n",
    "                full_text.append(para.text)\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" \".join(cell.text for cell in row.cells))\n",
    "\n",
    "        return \"\\n\".join(full_text)\n",
    "\n",
    "    def _is_header_footer(self, paragraph):\n",
    "        text = paragraph.text.lower().strip()\n",
    "        return any(keyword in text for keyword in \n",
    "                [\"header\", \"footer\", \"页码\", \"第\", \"页\"])\n",
    "\n",
    "    def format_text(self, text):\n",
    "        text = re.sub(r'\\r\\n?', '\\n', text)\n",
    "        text = '\\n'.join([line.strip() for line in text.split('\\n')])\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def deduplicate_text(self, text):\n",
    "        unique_lines = []\n",
    "        for line in text.split('\\n'):\n",
    "            line_hash = hashlib.md5(line.encode()).hexdigest()\n",
    "            if line_hash not in self.seen_hashes:\n",
    "                self.seen_hashes.add(line_hash)\n",
    "                unique_lines.append(line)\n",
    "        return '\\n'.join(unique_lines)\n",
    "\n",
    "    def get_synonyms(self, word):\n",
    "        return synonym_dict.get(word, [word])\n",
    "   \n",
    "    def augment_text(self, text, replace_ratio=0.3):\n",
    "        sentences = re.split(r'(?<=[。！？])', text)\n",
    "        augmented = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if len(sent) < 5:\n",
    "                augmented.append(sent)\n",
    "                continue\n",
    "            \n",
    "            words = list(sent)\n",
    "            for i in range(len(words)):\n",
    "                if random.random() < 0.2:\n",
    "                    syns = self.get_synonyms(words[i])\n",
    "                    if syns:\n",
    "                        words[i] = random.choice(syns)\n",
    "            sent = ''.join(words)\n",
    "            \n",
    "            augmented.append(sent)\n",
    "        \n",
    "        return ''.join(augmented)    \n",
    "\n",
    "    def process_file(self, input_path, output_path):\n",
    "        raw_text = self.read_docx(input_path)     \n",
    "\n",
    "        formatted = self.format_text(raw_text)        \n",
    "\n",
    "        deduped = self.deduplicate_text(formatted)        \n",
    "\n",
    "        augmented = self.augment_text(deduped)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"text\": augmented}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = DocProcessor()   \n",
    "    input_file = \"input_data.docx\"\n",
    "    output_file = \"processed_data.json\"    \n",
    "    processor.process_file(input_file, output_file)\n",
    "    print(f\"预处理输出文件：{output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evn_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
