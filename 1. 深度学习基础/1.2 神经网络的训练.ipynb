{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 神经网络的训练\n",
    "### 1.2.1 损失函数\n",
    "\n",
    "损失函数（Loss Function），也称为代价函数（Cost Function）或误差函数（Error Function），在机器学习和数学优化中扮演着至关重要的角色。它是一种衡量模型预测值与真实值之间差异的度量方式。简单来说，损失函数量化了给定模型对观测数据的拟合程度——模型预测的结果与实际观察结果之间的差距越大，损失函数的值就越高。\n",
    "常见的损失函数：均方误差、交叉熵损失\n",
    "\n",
    "#### 损失函数的作用\n",
    "\n",
    "1. **评估模型性能**：损失函数提供了一种方法来评估机器学习模型的预测准确性。通过最小化损失函数，我们可以找到最能准确反映输入数据与输出标签之间关系的模型参数。\n",
    "\n",
    "2. **指导模型训练**：在监督学习过程中，损失函数用于指导梯度下降等优化算法更新模型参数的方向和幅度，以便逐步减少预测误差，提高模型性能。\n",
    "\n",
    "#### 均方误差（Mean Squared Error, MSE）：\n",
    "常用于回归问题，计算每个样本预测值$\\hat{y}_i$与真实值$y_i$差的平方的平均值。\n",
    "  $$ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$\n",
    "  \n",
    "#### 交叉熵损失（Cross-Entropy Loss）：\n",
    "交叉熵（Cross-Entropy）是信息论中的一个重要概念，广泛应用于机器学习和深度学习中的分类任务。交叉熵损失（Cross-Entropy Loss）是衡量模型预测概率分布与真实概率分布之间差异的常用损失函数。\n",
    "\n",
    "##### 熵（Entropy）\n",
    "熵是衡量随机变量不确定性的指标，表示一个概率分布的不确定性或信息量。\n",
    "\n",
    "**（1）定义**\n",
    "对于一个离散随机变量 $X$，其概率分布为 $P(x)$，熵 $H(X)$ 定义为：\n",
    "\n",
    "$$H(X) = -\\sum_{x \\in X} P(x) \\log P(x)$$\n",
    "\n",
    "对于连续随机变量，熵的定义为：\n",
    "\n",
    "$$H(X) = -\\int_{x \\in X} p(x) \\log p(x) \\, dx$$\n",
    "\n",
    "**（2）性质**\n",
    "- 熵越大，表示随机变量的不确定性越高。\n",
    "- 熵的最小值为 0，当且仅当随机变量是确定的（即某个事件的概率为 1，其余为 0）。\n",
    "- 熵的最大值为 $ \\log N $，其中 $ N $ 是随机变量的可能取值数量，当且仅当所有事件的概率相等时达到最大值。\n",
    "\n",
    "**（3）示例**\n",
    "假设有一个离散随机变量 $ X $，其概率分布为 $ P = [0.5, 0.5] $，则熵为：\n",
    "\n",
    "$$\n",
    "H(X) = -(0.5 \\log 0.5 + 0.5 \\log 0.5) = -\\log 0.5 \\approx 0.693\n",
    "$$\n",
    "\n",
    "##### 交叉熵（Cross-Entropy）\n",
    "交叉熵用于衡量两个概率分布 $ P $ 和 $ Q $ 之间的差异，通常用于机器学习中的损失函数。\n",
    "\n",
    "**（1）定义**\n",
    "对于两个概率分布 $ P $ 和 $ Q $，交叉熵 $ H(P, Q) $ 定义为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\sum_{x \\in X} P(x) \\log Q(x)\n",
    "$$\n",
    "\n",
    "对于连续随机变量，交叉熵的定义为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\int_{x \\in X} p(x) \\log q(x) \\, dx\n",
    "$$\n",
    "\n",
    "**（2）性质**\n",
    "- 交叉熵越小，表示 $ Q $ 越接近 $ P $。\n",
    "- 当 $ Q = P $ 时，交叉熵等于熵，即 $ H(P, Q) = H(P) $。\n",
    "\n",
    "**（3）示例**\n",
    "假设真实分布 $ P = [1, 0] $，预测分布 $ Q = [0.8, 0.2] $，则交叉熵为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -(1 \\cdot \\log 0.8 + 0 \\cdot \\log 0.2) = -\\log 0.8 \\approx 0.2231\n",
    "$$\n",
    "\n",
    "如果预测分布 $ Q = [0.9, 0.8] $，则交叉熵为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -(1 \\cdot \\log 0.9 + 0 \\cdot \\log 0.8) = -\\log 0.9 \\approx 0.1053\n",
    "$$\n",
    "\n",
    "\n",
    "##### 散度（Divergence）\n",
    "散度用于衡量两个概率分布之间的差异，常用的散度包括 KL 散度和 JS 散度。\n",
    "\n",
    "**KL 散度（Kullback-Leibler Divergence）**\n",
    "KL 散度衡量 $ Q $ 分布与 $ P $ 分布之间的差异。\n",
    "\n",
    "**（1）定义**\n",
    "对于两个概率分布 $ P $ 和 $ Q $，KL 散度 $ D_{KL}(P \\parallel Q) $ 定义为：\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "对于连续随机变量，KL 散度的定义为：\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = \\int_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} \\, dx\n",
    "$$\n",
    "\n",
    "**（2）性质**\n",
    "- KL 散度是非负的，即 $ D_{KL}(P \\parallel Q) \\geq 0 $。\n",
    "- 当且仅当 $ P = Q $ 时，KL 散度为 0。\n",
    "- KL 散度不对称，即 $ D_{KL}(P \\parallel Q) \\neq D_{KL}(Q \\parallel P) $。\n",
    "\n",
    "**（3）示例**\n",
    "假设真实分布 $ P = [1, 0] $，预测分布 $ Q = [0.8, 0.2] $，则 KL 散度为：\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = 1 \\cdot \\log \\frac{1}{0.8} + 0 \\cdot \\log \\frac{0}{0.2} = \\log 1.25 \\approx 0.2231\n",
    "$$\n",
    "\n",
    "**JS散度（Jensen-Shannon Divergence）**\n",
    "JS 散度是对称的散度，基于 KL 散度定义。\n",
    "\n",
    "**（1）定义**\n",
    "对于两个概率分布 $ P $ 和 $ Q $，JS 散度 $ D_{JS}(P \\parallel Q) $ 定义为：\n",
    "\n",
    "$$\n",
    "D_{JS}(P \\parallel Q) = \\frac{1}{2} D_{KL}(P \\parallel M) + \\frac{1}{2} D_{KL}(Q \\parallel M)\n",
    "$$\n",
    "\n",
    "其中，$ M = \\frac{P + Q}{2} $ 是 $ P $ 和 $ Q $ 的平均分布。\n",
    "\n",
    "**（2）性质**\n",
    "- JS 散度是对称的，即 $ D_{JS}(P \\parallel Q) = D_{JS}(Q \\parallel P) $。\n",
    "- JS 散度的取值范围为 $ [0, 1] $。\n",
    "\n",
    "**（3）示例**\n",
    "假设真实分布 $ P = [1, 0] $，预测分布 $ Q = [0.8, 0.2] $，则 JS 散度为：\n",
    "\n",
    "- 计算平均分布 $ M $：\n",
    "   $$\n",
    "   M = \\frac{P + Q}{2} = [0.9, 0.1]\n",
    "   $$\n",
    "- 计算 KL 散度：\n",
    "   $$\n",
    "   D_{KL}(P \\parallel M) = 1 \\cdot \\log \\frac{1}{0.9} + 0 \\cdot \\log \\frac{0}{0.1} = \\log \\frac{1}{0.9} \\approx 0.1054\n",
    "   $$\n",
    "   $$\n",
    "   D_{KL}(Q \\parallel M) = 0.8 \\cdot \\log \\frac{0.8}{0.9} + 0.2 \\cdot \\log \\frac{0.2}{0.1} \\approx 0.8 \\cdot (-0.1178) + 0.2 \\cdot 0.6931 \\approx 0.0364\n",
    "   $$\n",
    "- 计算 JS 散度：\n",
    "   $$\n",
    "   D_{JS}(P \\parallel Q) = \\frac{1}{2} (0.1054 + 0.0364) \\approx 0.0709\n",
    "   $$\n",
    "\n",
    "\n",
    "**熵和交叉熵的关系**：交叉熵实际上可以视为熵加上额外的一个项，这个额外的项就是 KL散度。具体来说，对于两个分布 $P$ 和 $Q$，有 $H(P, Q) = H(P) + D_{KL}(P || Q)$。\n",
    "**KL散度和交叉熵的关系**：在机器学习实践中，特别是在训练分类模型时，我们通常最小化的是交叉熵而非直接最小化KL散度。这是因为交叉熵可以直接从模型输出和真实标签计算得到，并且最小化交叉熵等价于最小化KL散度（当 $P$ 固定时）。\n",
    "\n",
    "\n",
    "#### 机器学习中交叉熵损失的计算示例\n",
    "在分类任务中，真实分布 $ P $ 通常是**one-hot编码**的形式，即真实类别对应的概率为 1，其他类别为 0。假设有 $ C $ 个类别，真实类别为 $ y $，模型预测的概率分布为 $ \\hat{y} $，则交叉熵损失的计算步骤如下：\n",
    "\n",
    "**（1）真实分布 $ P $**\n",
    "真实分布 $ P $ 是一个 one-hot 向量，形式如下：\n",
    "\n",
    "$$\n",
    "P = [0, 0, \\dots, 1, \\dots, 0]\n",
    "$$\n",
    "\n",
    "其中，只有真实类别 $ y $ 对应的位置为 1，其余位置为 0。\n",
    "\n",
    "**（2）预测分布 $ Q $**\n",
    "模型输出的预测分布 $ Q $ 是一个概率向量，形式如下：\n",
    "\n",
    "$$\n",
    "Q = [q_1, q_2, \\dots, q_C]\n",
    "$$\n",
    "\n",
    "其中，$ q_i $ 表示模型预测的第 $ i $ 类的概率，且满足 $ \\sum_{i=1}^C q_i = 1 $。\n",
    "\n",
    "**（3）单样本交叉熵损失**\n",
    "由于真实分布 $ P $ 是 one-hot 编码，交叉熵损失可以简化为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\sum_{i=1}^C P(i) \\log Q(i) = -\\log Q(y)\n",
    "$$\n",
    "\n",
    "其中，$ Q(y) $ 是模型对真实类别 $ y $ 的预测概率。\n",
    "\n",
    "\n",
    "假设有一个 3 分类问题，真实类别为第 2 类，模型预测的概率分布为 $ Q = [0.1, 0.7, 0.2] $。\n",
    "\n",
    "- 真实分布 $ P $：\n",
    "  $$\n",
    "  P = [0, 1, 0]\n",
    "  $$\n",
    "- 交叉熵损失：\n",
    "  $$\n",
    "  H(P, Q) = -\\sum_{i=1}^3 P(i) \\log Q(i) = -(0 \\cdot \\log 0.1 + 1 \\cdot \\log 0.7 + 0 \\cdot \\log 0.2) = -\\log 0.7\n",
    "  $$\n",
    "- 计算结果：\n",
    "  $$\n",
    "  H(P, Q) \\approx -(-0.3567) = 0.3567\n",
    "  $$\n",
    "\n",
    "\n",
    "**（5）批量样本交叉熵损失**\n",
    "批量样本交叉熵损失的计算公式为：\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{j=1}^N \\sum_{i=1}^C P_j(i) \\log Q_j(i)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ N $ 是样本数量。\n",
    "- $ C $ 是类别数量。\n",
    "- $ P_j(i) $ 是第 $ j $ 个样本的真实分布（one-hot 编码）。\n",
    "- $ Q_j(i) $ 是第 $ j $ 个样本的预测概率分布。\n",
    "\n",
    "**示例**\n",
    "假设有 2 个样本，3 个类别：\n",
    "- 样本 1 的真实类别为第 2 类，预测概率为 $ [0.1, 0.7, 0.2] $。\n",
    "- 样本 2 的真实类别为第 3 类，预测概率为 $ [0.3, 0.4, 0.3] $。\n",
    "\n",
    "计算过程：\n",
    "- 样本 1 的损失：\n",
    "   $$\n",
    "   L_1 = -\\log 0.7 \\approx 0.3567\n",
    "   $$\n",
    "-  样本 2 的损失：\n",
    "   $$\n",
    "   L_2 = -\\log 0.3 \\approx 1.2040\n",
    "   $$\n",
    "- 平均损失：\n",
    "   $$\n",
    "   L = \\frac{L_1 + L_2}{2} = \\frac{0.3567 + 1.2040}{2} \\approx 0.7804\n",
    "   $$\n",
    "\n",
    "\n",
    "**（6）二分类的交叉熵损失**\n",
    "对于二分类问题，交叉熵损失的计算公式为：\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{j=1}^N \\left[ y_j \\log \\hat{y}_j + (1 - y_j) \\log (1 - \\hat{y}_j) \\right]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ y_j $ 是第 $ j $ 个样本的真实标签（0 或 1）。\n",
    "- $ \\hat{y}_j $ 是第 $ j $ 个样本的预测概率。\n",
    "\n",
    "**示例**\n",
    "假设有 2 个样本：\n",
    "- 样本 1 的真实标签为 1，预测概率为 0.8。\n",
    "- 样本 2 的真实标签为 0，预测概率为 0.3。\n",
    "\n",
    "计算过程：\n",
    "- 样本 1 的损失：\n",
    "   $$\n",
    "   L_1 = -\\left[ 1 \\cdot \\log 0.8 + (1 - 1) \\cdot \\log (1 - 0.8) \\right] = -\\log 0.8 \\approx 0.2231\n",
    "   $$\n",
    "- 样本 2 的损失：\n",
    "   $$\n",
    "   L_2 = -\\left[ 0 \\cdot \\log 0.3 + (1 - 0) \\cdot \\log (1 - 0.3) \\right] = -\\log 0.7 \\approx 0.3567\n",
    "   $$\n",
    "- 平均损失：\n",
    "   $$\n",
    "   L = \\frac{L_1 + L_2}{2} = \\frac{0.2231 + 0.3567}{2} \\approx 0.2899\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE计算示例\n",
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "# 设“2”为正确解\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "mse = mean_squared_error(np.array(y), np.array(t))\n",
    "print(mse)\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mse = mean_squared_error(np.array(y), np.array(t))    \n",
    "print(mse)\n",
    "\n",
    "# output\n",
    "# 0.09750000000000003\n",
    "# 0.5975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵计算示例\n",
    "# 单样本计算\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "t1 = [0, 1, 0]\n",
    "y1 = [0.1, 0.7, 0.2]\n",
    "loss1 = cross_entropy_error(np.array(y1), np.array(t1))\n",
    "t2 = [0, 0, 1]\n",
    "y2 = [0.3, 0.4, 0.3]\n",
    "loss2 = cross_entropy_error(np.array(y2), np.array(t2))\n",
    "loss = (loss1+loss2)/2.0\n",
    "print(loss)\n",
    "\n",
    "# 批量样本计算\n",
    "def batch_cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)        \n",
    "        y = y.reshape(1, y.size)        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "t = np.array([[0,1,0],[0,0,1]])\n",
    "y = np.array([[0.1, 0.7, 0.2],[0.3, 0.4, 0.3]])\n",
    "loss = batch_cross_entropy_error(y, t)\n",
    "print(loss)\n",
    "# output\n",
    "# 0.7803236360371291\n",
    "# 0.7803236360371291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 梯度\n",
    "####  **梯度（Gradient）**\n",
    "梯度是一个向量，表示函数在某一点处的方向导数沿着各个坐标轴的分量。对于多元函数 $ f(\\mathbf{x}) $，其梯度定义为：\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right]^T\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ \\mathbf{x} = [x_1, x_2, \\dots, x_n]^T $ 是函数的自变量。\n",
    "- $ \\frac{\\partial f}{\\partial x_i} $ 是函数 $ f $ 对 $ x_i $ 的偏导数。\n",
    "\n",
    "**（1）梯度的意义**\n",
    "- 梯度指向函数值增长最快的方向。\n",
    "- 梯度的模长表示函数值变化的速率。\n",
    "\n",
    "**（2）示例**\n",
    "假设函数 $ f(x, y) = x^2 + y^2 $，其梯度为：\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y) = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right]^T = [2x, 2y]^T\n",
    "$$\n",
    "\n",
    "在点 $ (1, 2) $ 处的梯度为：\n",
    "\n",
    "$$\n",
    "\\nabla f(1, 2) = [2 \\cdot 1, 2 \\cdot 2]^T = [2, 4]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    \"\"\"\n",
    "    计算函数 f(x, y) = x^2 + y^2 在点 (x, y) 处的梯度\n",
    "    \n",
    "    参数:\n",
    "    x: float 或 array-like, 点的 x 坐标\n",
    "    y: float 或 array-like, 点的 y 坐标\n",
    "    \n",
    "    返回:\n",
    "    grad: array-like, 梯度向量 (df/dx, df/dy)\n",
    "    \"\"\"\n",
    "    df_dx = 2 * x\n",
    "    df_dy = 2 * y\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# 示例点\n",
    "x_example = 3\n",
    "y_example = 4\n",
    "\n",
    "# 计算梯度\n",
    "grad_example = gradient_f(x_example, y_example)\n",
    "print(f\"Gradient at point ({x_example}, {y_example}): {grad_example}\")\n",
    "\n",
    "# 可视化梯度场\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建网格\n",
    "x_vals = np.linspace(-5, 5, 20)\n",
    "y_vals = np.linspace(-5, 5, 20)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# 计算每个点的梯度\n",
    "U, V = gradient_f(X, Y)\n",
    "\n",
    "# 绘制梯度场\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.quiver(X, Y, U, V, color='blue')\n",
    "plt.title('Gradient Field of $f(x, y) = x^2 + y^2$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.axhline(0, color='black',linewidth=0.5)\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **梯度下降法（Gradient Descent）**\n",
    "梯度下降法是一种迭代优化算法，通过沿着梯度的反方向更新参数，逐步逼近函数的最小值。\n",
    "\n",
    "**（1）算法步骤**\n",
    "- step1初始化参数：选择初始点 $ \\mathbf{x}_0 $。\n",
    "- step2计算梯度：在当前点 $ \\mathbf{x}_k $ 计算梯度 $ \\nabla f(\\mathbf{x}_k) $。\n",
    "- step3更新参数：沿着梯度的反方向更新参数：\n",
    "   $$\n",
    "   \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\eta \\nabla f(\\mathbf{x}_k)\n",
    "   $$\n",
    "   其中，$ \\eta $ 是学习率（步长）。\n",
    "- step4重复迭代：重复step2 和 step3，直到满足停止条件（如梯度接近零或达到最大迭代次数）。\n",
    "\n",
    "**（2）学习率 $ \\eta $**\n",
    "- 学习率控制每次更新的步长。\n",
    "- 学习率过大会导致震荡或发散，过小会导致收敛速度慢。\n",
    "\n",
    "**（3）计算示例**\n",
    "\n",
    "函数 $ f(x, y) = x^2 + y^2 $ 的梯度为：\n",
    "$$\n",
    "\\nabla f(x, y) = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right]^T = [2x, 2y]^T\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**具体迭代过程**\n",
    "假设初始点为 $ \\mathbf{x}_0 = [3, 4]^T $，学习率 $ \\eta = 0.1 $。\n",
    "\n",
    "**（1）第一次迭代**\n",
    "- 计算梯度：\n",
    "  $$\n",
    "  \\nabla f(\\mathbf{x}_0) = [2 \\cdot 3, 2 \\cdot 4]^T = [6, 8]^T\n",
    "  $$\n",
    "- 更新参数：\n",
    "  $$\n",
    "  \\mathbf{x}_1 = \\mathbf{x}_0 - \\eta \\nabla f(\\mathbf{x}_0) = [3, 4]^T - 0.1 \\cdot [6, 8]^T = [3 - 0.6, 4 - 0.8]^T = [2.4, 3.2]^T\n",
    "  $$\n",
    "\n",
    "**（2）第二次迭代**\n",
    "- 计算梯度：\n",
    "  $$\n",
    "  \\nabla f(\\mathbf{x}_1) = [2 \\cdot 2.4, 2 \\cdot 3.2]^T = [4.8, 6.4]^T\n",
    "  $$\n",
    "- 更新参数：\n",
    "  $$\n",
    "  \\mathbf{x}_2 = \\mathbf{x}_1 - \\eta \\nabla f(\\mathbf{x}_1) = [2.4, 3.2]^T - 0.1 \\cdot [4.8, 6.4]^T = [2.4 - 0.48, 3.2 - 0.64]^T = [1.92, 2.56]^T\n",
    "  $$\n",
    "\n",
    "**（3）第三次迭代**\n",
    "- 计算梯度：\n",
    "  $$\n",
    "  \\nabla f(\\mathbf{x}_2) = [2 \\cdot 1.92, 2 \\cdot 2.56]^T = [3.84, 5.12]^T\n",
    "  $$\n",
    "- 更新参数：\n",
    "  $$\n",
    "  \\mathbf{x}_3 = \\mathbf{x}_2 - \\eta \\nabla f(\\mathbf{x}_2) = [1.92, 2.56]^T - 0.1 \\cdot [3.84, 5.12]^T = [1.92 - 0.384, 2.56 - 0.512]^T = [1.536, 2.048]^T\n",
    "  $$\n",
    "\n",
    "**（4）第四次迭代**\n",
    "- 计算梯度：\n",
    "  $$\n",
    "  \\nabla f(\\mathbf{x}_3) = [2 \\cdot 1.536, 2 \\cdot 2.048]^T = [3.072, 4.096]^T\n",
    "  $$\n",
    "- 更新参数：\n",
    "  $$\n",
    "  \\mathbf{x}_4 = \\mathbf{x}_3 - \\eta \\nabla f(\\mathbf{x}_3) = [1.536, 2.048]^T - 0.1 \\cdot [3.072, 4.096]^T = [1.536 - 0.3072, 2.048 - 0.4096]^T = [1.2288, 1.6384]^T\n",
    "  $$\n",
    "\n",
    "**（5）第五次迭代**\n",
    "- 计算梯度：\n",
    "  $$\n",
    "  \\nabla f(\\mathbf{x}_4) = [2 \\cdot 1.2288, 2 \\cdot 1.6384]^T = [2.4576, 3.2768]^T\n",
    "  $$\n",
    "- 更新参数：\n",
    "  $$\n",
    "  \\mathbf{x}_5 = \\mathbf{x}_4 - \\eta \\nabla f(\\mathbf{x}_4) = [1.2288, 1.6384]^T - 0.1 \\cdot [2.4576, 3.2768]^T = [1.2288 - 0.24576, 1.6384 - 0.32768]^T = [0.98304, 1.31072]^T\n",
    "  $$\n",
    "\n",
    "**迭代结果汇总**\n",
    "\n",
    "| 迭代次数 $ k $ | $ x_k $    | $ y_k $    | 梯度 $ \\nabla f(x_k, y_k) $ | 更新后的 $ x_{k+1} $ | 更新后的 $ y_{k+1} $ |\n",
    "|------------------|--------------|--------------|-------------------------------|------------------------|------------------------|\n",
    "| 0                | 3.0          | 4.0          | [6.0, 8.0]                    | 2.4                    | 3.2                    |\n",
    "| 1                | 2.4          | 3.2          | [4.8, 6.4]                    | 1.92                   | 2.56                   |\n",
    "| 2                | 1.92         | 2.56         | [3.84, 5.12]                  | 1.536                  | 2.048                  |\n",
    "| 3                | 1.536        | 2.048        | [3.072, 4.096]                | 1.2288                 | 1.6384                 |\n",
    "| 4                | 1.2288       | 1.6384       | [2.4576, 3.2768]              | 0.98304                | 1.31072                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"函数的梯度下降示例\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义函数 f(x, y) = x^2 + y^2\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# 定义梯度函数 ∇f(x, y) = (2x, 2y)\n",
    "def gradient_f(x, y):\n",
    "    df_dx = 2 * x\n",
    "    df_dy = 2 * y\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# 超参数设置\n",
    "initial_point = np.array([3.0, 4.0])  # 初始点 (x0, y0)\n",
    "learning_rate = 0.1                  # 学习率\n",
    "n_iterations = 100                   # 迭代次数\n",
    "\n",
    "# 初始化参数\n",
    "theta = initial_point.copy()\n",
    "\n",
    "# 存储每次迭代的结果以便绘图\n",
    "points_history = [theta.copy()]\n",
    "\n",
    "# 梯度下降迭代过程\n",
    "for iteration in range(n_iterations):\n",
    "    grad = gradient_f(theta[0], theta[1])\n",
    "    theta -= learning_rate * grad\n",
    "    points_history.append(theta.copy())\n",
    "\n",
    "    # 打印每次迭代的结果\n",
    "    print(f\"Iteration {iteration + 1}: theta = {theta}, f(theta) = {f(theta[0], theta[1])}\")\n",
    "\n",
    "# 绘制结果\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 创建网格\n",
    "x_vals = np.linspace(-5, 5, 100)\n",
    "y_vals = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# 计算每个点的函数值\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "# 绘制等高线图\n",
    "contour_levels = np.logspace(-1, 3, 20)\n",
    "plt.contour(X, Y, Z, levels=contour_levels, cmap='viridis')\n",
    "\n",
    "# 绘制梯度下降路径\n",
    "points_history = np.array(points_history)\n",
    "plt.plot(points_history[:, 0], points_history[:, 1], 'ro-', markersize=5, label='Gradient Descent Path')\n",
    "plt.scatter(initial_point[0], initial_point[1], color='red', s=100, zorder=5, label='Initial Point')\n",
    "plt.scatter(0, 0, color='green', s=100, zorder=5, label='Global Minimum (0, 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 链式法则和计算图\n",
    "\n",
    "#### 链式法则（Chain Rule）\n",
    "\n",
    "链式法则是微积分中的一个基本法则，用于计算复合函数的导数。假设有一个函数 $y = f(g(x))$，这里 $f$ 和 $g$ 是两个可导函数，$x$ 是输入变量，$g(x)$ 是中间变量，而 $f$ 是作用于 $g(x)$ 的外层函数。根据链式法则，$y$ 对 $x$ 的导数可以表示为：\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx} $$\n",
    "\n",
    "\n",
    "当涉及到多元函数时，链式法则变得更加复杂。考虑一个多元函数 $z = f(u_1, u_2, ..., u_n)$，其中每个 $u_i$ 都是关于 $x$ 的函数。在这种情况下，$z$ 对 $x$ 的导数可以表示为：\n",
    "\n",
    "$$ \\frac{dz}{dx} = \\sum_{i=1}^{n} \\left( \\frac{\\partial z}{\\partial u_i} \\cdot \\frac{du_i}{dx} \\right) $$\n",
    "\n",
    "\n",
    "深度学习的模型通常由多层非线性变换组成。这些变换可以被视作一系列函数的组合，通过链式法则，可以有效地计算出损失函数相对于每一层参数的梯度。这种方法被称为反向传播（Backpropagation），它是训练神经网络的核心技术之一。神经网络中的梯度计算是反向传播算法（Backpropagation）的核心部分。反向传播通过链式法则计算损失函数对每个参数的梯度，然后利用梯度下降法更新参数。\n",
    "\n",
    "#### 计算图（Computation Graph）\n",
    "计算图是描述函数计算过程的有向无环图（DAG）。下面以函数 $ f = (2x + 3y)^2 $ 为例，说明计算图的计算过程。\n",
    "\n",
    "**（1）输入**：$ x $ 和 $ y $。\n",
    "**（2）中间节点**：\n",
    "   - $ u = 2x $（$ x $ 乘以 2）。\n",
    "   - $ v = 3y $（$ y $ 乘以 3）。\n",
    "   - $ z = u + v $（$ u $ 和 $ v $ 相加）。\n",
    "   - $ f = z^2 $（$ z $ 的平方）。\n",
    "\n",
    "计算图如下：\n",
    "\n",
    "<img src=\"./images/c_graph_2x3y.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "**前向传播计算**\n",
    "前向传播是沿着计算图从输入到输出计算函数值的过程（假设 $ x = 1 $，$ y = 2 $）。\n",
    "（1） $ u = 2x = 2 \\cdot 1 = 2 $\n",
    "（2） $ v = 3y  = 3 \\cdot 2 = 6 $\n",
    "（3） $ z = u + v = 2 + 6 = 8 $。\n",
    "（4） $ f = z^2 = 8^2 = 64 $。\n",
    "\n",
    "**反向传播计算**\n",
    "反向传播是通过链式法则从输出到输入计算梯度的过程。\n",
    "对于函数 $ f = z^2 $，其中 $ z = u + v $，$ u = 2x $，$ v = 3y $，梯度的计算如下（前向传播结果：$ z = 8 $，$ f = 64 $）：\n",
    "\n",
    "(1) **计算 $ \\frac{\\partial f}{\\partial z} $**：\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial z} = 2z = 2 \\cdot 8 = 16\n",
    "   $$\n",
    "\n",
    "(2) **计算 $ \\frac{\\partial z}{\\partial u} $ 和 $ \\frac{\\partial z}{\\partial v} $**：\n",
    "   $$\n",
    "   \\frac{\\partial z}{\\partial u} = 1, \\quad \\frac{\\partial z}{\\partial v} = 1\n",
    "   $$\n",
    "\n",
    "(3) **计算 $ \\frac{\\partial f}{\\partial u} $ 和 $ \\frac{\\partial f}{\\partial v} $**：\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial u} = \\frac{\\partial f}{\\partial z} \\cdot \\frac{\\partial z}{\\partial u} = 2z \\cdot 1 = 2z= 16\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial v} = \\frac{\\partial f}{\\partial z} \\cdot \\frac{\\partial z}{\\partial v} = 2z \\cdot 1 = 2z= 16\n",
    "   $$\n",
    "\n",
    "(4) **计算 $ \\frac{\\partial u}{\\partial x} $ 和 $ \\frac{\\partial v}{\\partial y} $**：\n",
    "   $$\n",
    "   \\frac{\\partial u}{\\partial x} = 2, \\quad \\frac{\\partial v}{\\partial y} = 3\n",
    "   $$\n",
    "\n",
    "(5) **计算 $ \\frac{\\partial f}{\\partial x} $ 和 $ \\frac{\\partial f}{\\partial y} $**：\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x} = 2z \\cdot 2 = 16 \\cdot 2 = 32\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial v} \\cdot \\frac{\\partial v}{\\partial y} = 2z \\cdot 3 = 16 \\cdot 3 = 48\n",
    "   $$\n",
    "\n",
    "以下是函数 $ f = (2x + 3y)^2 $ 的计算图和梯度反向传播的Python实现：\n",
    "\n",
    "运行上述代码后，输出结果如下：\n",
    "\n",
    "```\n",
    "Forward: f = 64, z = 8, u = 2, v = 6\n",
    "Backward: df/dx = 32, df/dy = 48\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 前向传播\n",
    "def forward(x, y):\n",
    "    u = 2 * x\n",
    "    v = 3 * y\n",
    "    z = u + v\n",
    "    f = z ** 2\n",
    "    return f, z, u, v\n",
    "\n",
    "# 反向传播\n",
    "def backward(x, y, z):\n",
    "    dz = 2 * z\n",
    "    du = dz * 1  # dz/du = 1\n",
    "    dv = dz * 1  # dz/dv = 1\n",
    "    dx = du * 2  # du/dx = 2\n",
    "    dy = dv * 3  # dv/dy = 3\n",
    "    return dx, dy\n",
    "\n",
    "# 输入\n",
    "x = 1\n",
    "y = 2\n",
    "\n",
    "# 前向传播\n",
    "f, z, u, v = forward(x, y)\n",
    "print(f\"Forward: f = {f}, z = {z}, u = {u}, v = {v}\")\n",
    "\n",
    "# 反向传播\n",
    "df_dx, df_dy = backward(x, y, z)\n",
    "print(f\"Backward: df/dx = {df_dx}, df/dy = {df_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络的梯度和反向传播\n",
    "\n",
    "**简单的两层神经网络**\n",
    "- 输入层：$ \\mathbf{x} = [x_1, x_2]^T $。\n",
    "- 隐藏层：使用 Sigmoid 激活函数。\n",
    "- 输出层：使用 Softmax 正则化。\n",
    "- 损失函数：交叉熵损失。\n",
    "\n",
    "**前向传播**\n",
    "前向传播计算每一层的输出和损失函数。\n",
    "\n",
    "**（1）隐藏层**\n",
    "- 输入：$ \\mathbf{x} $。\n",
    "- 加权输入：\n",
    "  $$\n",
    "  \\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1\n",
    "  $$\n",
    "- 激活输出：\n",
    "  $$\n",
    "  \\mathbf{h} = \\sigma(\\mathbf{z}_1)\n",
    "  $$\n",
    "  其中，Sigmoid 函数为：\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "\n",
    "**（2）输出层**\n",
    "- 加权输入：\n",
    "  $$\n",
    "  \\mathbf{z}_2 = \\mathbf{W}_2 \\mathbf{h} + \\mathbf{b}_2\n",
    "  $$\n",
    "- 激活输出：\n",
    "  $$\n",
    "  \\mathbf{y} = \\text{Softmax}(\\mathbf{z}_2)\n",
    "  $$\n",
    "  其中，Softmax 函数为：\n",
    "  $$\n",
    "  y_i = \\frac{e^{z_{2,i}}}{\\sum_{j} e^{z_{2,j}}}\n",
    "  $$\n",
    "\n",
    "**（3）损失函数**\n",
    "- 交叉熵损失：\n",
    "  $$\n",
    "  L = -\\sum_{i} t_i \\log y_i\n",
    "  $$\n",
    "\n",
    "   其中，$ \\mathbf{t} = [t_1, t_2, \\dots, t_C]^T $ 是目标值（one-hot 编码），$ \\mathbf{y} = [y_1, y_2, \\dots, y_C]^T $ 是输出层的预测概率。\n",
    "\n",
    "**反向传播**\n",
    "反向传播通过链式法则计算损失函数对每个参数的梯度。\n",
    "\n",
    "**（1）输出层梯度**\n",
    "- **损失函数对 Softmax 输出的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial y_i} = -\\frac{t_i}{y_i}\n",
    "   $$\n",
    "- **Softmax 对加权输入的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial y_i}{\\partial z_{2,j}} = y_i (\\delta_{ij} - y_j)\n",
    "   $$\n",
    "   其中，$ \\delta_{ij} $ 是 Kronecker delta 函数（当 $ i = j $ 时为 1，否则为 0）。\n",
    "- **损失函数对 $ \\mathbf{z}_2 $ 的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial z_{2,i}} = \\sum_{j} \\frac{\\partial L}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial z_{2,i}} = y_i - t_i\n",
    "   $$\n",
    "- **损失函数对 $ \\mathbf{W}_2 $ 和 $ \\mathbf{b}_2 $ 的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{W}_2} = \\frac{\\partial L}{\\partial \\mathbf{z}_2} \\cdot \\mathbf{h}^T\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{b}_2} = \\frac{\\partial L}{\\partial \\mathbf{z}_2}\n",
    "   $$\n",
    "\n",
    "**（2）隐藏层梯度**\n",
    "- **损失函数对 $ \\mathbf{h} $ 的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{h}} = \\mathbf{W}_2^T \\cdot \\frac{\\partial L}{\\partial \\mathbf{z}_2}\n",
    "   $$\n",
    "- **Sigmoid 对加权输入的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial h_i}{\\partial z_{1,i}} = h_i (1 - h_i)\n",
    "   $$\n",
    "- **损失函数对 $ \\mathbf{z}_1 $ 的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{z}_1} = \\frac{\\partial L}{\\partial \\mathbf{h}} \\odot \\sigma'(\\mathbf{z}_1)\n",
    "   $$\n",
    "- **损失函数对 $ \\mathbf{W}_1 $ 和 $ \\mathbf{b}_1 $ 的梯度**：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{W}_1} = \\frac{\\partial L}{\\partial \\mathbf{z}_1} \\cdot \\mathbf{x}^T\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{b}_1} = \\frac{\\partial L}{\\partial \\mathbf{z}_1}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Output:\n",
      "y = [[0.99861911]\n",
      " [0.00138089]]\n",
      "Target = [[1]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Softmax 函数\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # 防止数值溢出\n",
    "    return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "# 交叉熵损失函数\n",
    "def cross_entropy_loss(y, t):\n",
    "    return -np.sum(t * np.log(y))\n",
    "\n",
    "# 初始化参数\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 2\n",
    "\n",
    "# 随机初始化权重和偏置\n",
    "W1 = np.random.randn(hidden_size, input_size)  # 隐藏层权重 (3x2)\n",
    "b1 = np.random.randn(hidden_size, 1)          # 隐藏层偏置 (3x1)\n",
    "W2 = np.random.randn(output_size, hidden_size) # 输出层权重 (2x3)\n",
    "b2 = np.random.randn(output_size, 1)          # 输出层偏置 (2x1)\n",
    "\n",
    "# 输入和目标值\n",
    "x = np.array([[1], [2]])  # 输入 (2x1)\n",
    "t = np.array([[1], [0]])  # 目标值 (one-hot 编码) (2x1)\n",
    "\n",
    "# 前向传播\n",
    "def forward(x, W1, b1, W2, b2):\n",
    "    # 隐藏层\n",
    "    z1 = np.dot(W1, x) + b1   # 加权输入 (3x1)\n",
    "    h = sigmoid(z1)           # 激活输出 (3x1)\n",
    "\n",
    "    # 输出层\n",
    "    z2 = np.dot(W2, h) + b2   # 加权输入 (2x1)\n",
    "    y = softmax(z2)           # 输出正则化 (2x1)\n",
    "\n",
    "    return y, h, z1, z2\n",
    "\n",
    "# 反向传播\n",
    "def backward(x, t, y, h, z1, z2, W2):\n",
    "    # 输出层梯度\n",
    "    dL_dz2 = y - t            # 交叉熵损失对 z2 的梯度 (2x1)\n",
    "\n",
    "    # W2 和 b2 的梯度\n",
    "    dL_dW2 = np.dot(dL_dz2, h.T)  # (2x3)\n",
    "    dL_db2 = dL_dz2           # (2x1)\n",
    "\n",
    "    # 隐藏层梯度\n",
    "    dL_dh = np.dot(W2.T, dL_dz2)  # (3x1)\n",
    "    dL_dz1 = dL_dh * sigmoid_derivative(z1)  # (3x1)\n",
    "\n",
    "    # W1 和 b1 的梯度\n",
    "    dL_dW1 = np.dot(dL_dz1, x.T)  # (3x2)\n",
    "    dL_db1 = dL_dz1           # (3x1)\n",
    "\n",
    "    return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "\n",
    "# 训练参数\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "\n",
    "# 记录损失\n",
    "loss_history = []\n",
    "\n",
    "# 训练过程\n",
    "for i in range(num_iterations):\n",
    "    # 前向传播\n",
    "    y, h, z1, z2 = forward(x, W1, b1, W2, b2)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = cross_entropy_loss(y, t)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # 反向传播\n",
    "    dL_dW1, dL_db1, dL_dW2, dL_db2 = backward(x, t, y, h, z1, z2, W2)\n",
    "\n",
    "    # 更新参数\n",
    "    W1 -= learning_rate * dL_dW1\n",
    "    b1 -= learning_rate * dL_db1\n",
    "    W2 -= learning_rate * dL_dW2\n",
    "    b2 -= learning_rate * dL_db2\n",
    "\n",
    "# 最终输出\n",
    "print(\"\\nFinal Output:\")\n",
    "print(\"y =\", y)\n",
    "print(\"Target =\", t)\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(loss_history)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
