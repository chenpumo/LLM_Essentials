{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 分词器算法\n",
    "\n",
    "### BPE（Byte Pair Encoding）\n",
    "\n",
    "BPE（Byte Pair Encoding）是一种数据压缩算法，后来被广泛应用于自然语言处理中的分词任务。BPE通过逐步合并最常见的字符对来构建词汇表，从而将单词分解为子词单元。这种方法能够有效地处理未登录词（OOV）问题，并且在处理稀有词汇时表现良好。\n",
    "\n",
    "#### BPE算法的基本步骤\n",
    "\n",
    "1. **初始化词汇表**：\n",
    "   - 将每个单词分解为字符序列，并将这些字符作为初始词汇表。\n",
    "\n",
    "2. **统计字符对频率**：\n",
    "   - 统计所有相邻字符对的出现频率。\n",
    "\n",
    "3. **合并最常见字符对**：\n",
    "   - 找到出现频率最高的字符对，并将其合并为一个新的符号，更新词汇表。\n",
    "\n",
    "4. **重复合并过程**：\n",
    "   - 重复步骤2和步骤3，直到达到预定的词汇表大小或合并次数。\n",
    "\n",
    "#### 示例\n",
    "\n",
    "假设有以下文本数据：\n",
    "\n",
    "```\n",
    "\"low lower lowest\"\n",
    "```\n",
    "\n",
    "#### 初始词汇表\n",
    "\n",
    "首先，我们将每个单词分解为字符序列，并统计每个字符的出现频率：\n",
    "\n",
    "```\n",
    "l, o, w,   l, o, w, e, r,   l, o, w, e, s, t\n",
    "```\n",
    "\n",
    "初始词汇表为：`{'l', 'o', 'w', 'e', 'r', 's', 't'}`\n",
    "\n",
    "#### 第一次合并\n",
    "\n",
    "统计所有相邻字符对的出现频率：\n",
    "\n",
    "```\n",
    "lo: 3\n",
    "ow: 3\n",
    "we: 1\n",
    "er: 1\n",
    "es: 1\n",
    "st: 1\n",
    "```\n",
    "\n",
    "最常见的字符对是`lo`和`ow`，假设我们选择合并`lo`。合并后，词汇表更新为：`{'lo', 'w', 'e', 'r', 's', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"lo w lo w e r lo w e s t\"\n",
    "```\n",
    "\n",
    "#### 第二次合并\n",
    "\n",
    "再次统计相邻字符对的出现频率：\n",
    "\n",
    "```\n",
    "lo w: 3\n",
    "w e: 1\n",
    "e r: 1\n",
    "e s: 1\n",
    "s t: 1\n",
    "```\n",
    "\n",
    "最常见的字符对是`lo w`，我们将其合并为`low`。词汇表更新为：`{'low', 'e', 'r', 's', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low low e r low e s t\"\n",
    "```\n",
    "\n",
    "#### 第三次合并\n",
    "\n",
    "统计相邻字符对的出现频率：\n",
    "\n",
    "```\n",
    "low e: 2\n",
    "e r: 1\n",
    "e s: 1\n",
    "s t: 1\n",
    "```\n",
    "\n",
    "最常见的字符对是`low e`，我们将其合并为`lowe`。词汇表更新为：`{'low', 'lowe', 'r', 's', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low lowe r lowe s t\"\n",
    "```\n",
    "\n",
    "#### 第四次合并\n",
    "\n",
    "统计相邻字符对的出现频率：\n",
    "\n",
    "```\n",
    "low r: 1\n",
    "r lowe: 1\n",
    "lowe s: 1\n",
    "s t: 1\n",
    "```\n",
    "\n",
    "最常见的字符对是`lowe s`，我们将其合并为`lowes`。词汇表更新为：`{'low', 'lowe', 'lowes', 'r', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low lowe r lowes t\"\n",
    "```\n",
    "\n",
    "#### 第五次合并\n",
    "\n",
    "统计相邻字符对的出现频率：\n",
    "\n",
    "```\n",
    "low r: 1\n",
    "r lowes: 1\n",
    "lowes t: 1\n",
    "```\n",
    "\n",
    "最常见的字符对是`lowes t`，我们将其合并为`lowest`。词汇表更新为：`{'low', 'lowe', 'lowes', 'lowest', 'r'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low lowe r lowest\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary: {'low': 1, 'lower': 1, 'lowest': 1, 'good': 1, 'god': 1, 'goods': 1, 'new': 1, 'news': 1}\n"
     ]
    }
   ],
   "source": [
    "### BPE算法\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in vocab_in:\n",
    "        w_out = word.replace(bigram, replacement)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out\n",
    "\n",
    "def bpe_tokenize(text, num_merges):\n",
    "    # 初始化词汇表\n",
    "    vocab = {' '.join(word): freq for word, freq in Counter(text.split()).items()}\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# 示例文本\n",
    "text = \"low lower lowest good god goods new news\"\n",
    "\n",
    "# 设置合并次数\n",
    "num_merges = 20\n",
    "\n",
    "# 执行BPE算法\n",
    "vocab = bpe_tokenize(text, num_merges)\n",
    "\n",
    "# 输出最终的词汇表\n",
    "print(\"Final Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece\n",
    "WordPiece 是一种广泛应用于自然语言处理中的分词算法，特别是在BERT等预训练语言模型中。WordPiece 与 BPE（Byte Pair Encoding）类似，但有一些关键区别。WordPiece 的目标是通过最大化语言模型的对数似然来选择最佳的子词单元，而不是简单地基于字符对频率进行合并。\n",
    "\n",
    "#### WordPiece 算法的基本步骤\n",
    "\n",
    "1. **初始化词汇表**：\n",
    "   - 将每个单词分解为字符序列，并将这些字符作为初始词汇表。\n",
    "\n",
    "2. **训练语言模型**：\n",
    "   - 使用初始词汇表训练一个语言模型，计算每个子词单元的概率。\n",
    "\n",
    "3. **选择最佳合并**：\n",
    "   - 选择能够最大化语言模型对数似然的字符对进行合并。\n",
    "\n",
    "4. **更新词汇表**：\n",
    "   - 将合并后的新子词单元加入词汇表。\n",
    "\n",
    "5. **重复合并过程**：\n",
    "   - 重复步骤2到步骤4，直到达到预定的词汇表大小或合并次数。\n",
    "\n",
    "#### 示例\n",
    "\n",
    "假设有以下文本数据：\n",
    "\n",
    "```\n",
    "\"low lower lowest\"\n",
    "```\n",
    "\n",
    "#### 初始词汇表\n",
    "\n",
    "首先，我们将每个单词分解为字符序列，并统计每个字符的出现频率：\n",
    "\n",
    "```\n",
    "l, o, w,   l, o, w, e, r,   l, o, w, e, s, t\n",
    "```\n",
    "\n",
    "初始词汇表为：`{'l', 'o', 'w', 'e', 'r', 's', 't'}`\n",
    "\n",
    "#### 第一次合并\n",
    "\n",
    "假设我们训练了一个语言模型，并计算了每个字符对的对数似然。假设`lo`的对数似然最高，我们选择合并`lo`。合并后，词汇表更新为：`{'lo', 'w', 'e', 'r', 's', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"lo w lo w e r lo w e s t\"\n",
    "```\n",
    "\n",
    "#### 第二次合并\n",
    "\n",
    "再次训练语言模型，并计算每个字符对的对数似然。假设`low`的对数似然最高，我们选择合并`low`。词汇表更新为：`{'low', 'e', 'r', 's', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low low e r low e s t\"\n",
    "```\n",
    "\n",
    "#### 第三次合并\n",
    "\n",
    "再次训练语言模型，并计算每个字符对的对数似然。假设`lowe`的对数似然最高，我们选择合并`lowe`。词汇表更新为：`{'low', 'lowe', 'r', 's', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low lowe r lowe s t\"\n",
    "```\n",
    "\n",
    "#### 第四次合并\n",
    "\n",
    "再次训练语言模型，并计算每个字符对的对数似然。假设`lowes`的对数似然最高，我们选择合并`lowes`。词汇表更新为：`{'low', 'lowe', 'lowes', 'r', 't'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low lowe r lowes t\"\n",
    "```\n",
    "\n",
    "#### 第五次合并\n",
    "\n",
    "再次训练语言模型，并计算每个字符对的对数似然。假设`lowest`的对数似然最高，我们选择合并`lowest`。词汇表更新为：`{'low', 'lowe', 'lowes', 'lowest', 'r'}`\n",
    "\n",
    "更新后的文本表示：\n",
    "\n",
    "```\n",
    "\"low lowe r lowest\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary: {'low': 1, 'lower': 1, 'lowest': 1}\n"
     ]
    }
   ],
   "source": [
    "### WordPiece算法\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in vocab_in:\n",
    "        w_out = word.replace(bigram, replacement)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out\n",
    "\n",
    "def compute_log_likelihood(vocab, pair):\n",
    "    # 这里简化了计算，实际应用中需要使用语言模型来计算对数似然\n",
    "    return math.log(sum(vocab.values()))\n",
    "\n",
    "def wordpiece_tokenize(text, num_merges):\n",
    "    # 初始化词汇表\n",
    "    vocab = {' '.join(word): freq for word, freq in Counter(text.split()).items()}\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # 选择能够最大化对数似然的字符对\n",
    "        best_pair = max(pairs, key=lambda p: compute_log_likelihood(vocab, p))\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# 示例文本\n",
    "text = \"low lower lowest\"\n",
    "\n",
    "# 设置合并次数\n",
    "num_merges = 10\n",
    "\n",
    "# 执行WordPiece算法\n",
    "vocab = wordpiece_tokenize(text, num_merges)\n",
    "\n",
    "# 输出最终的词汇表\n",
    "print(\"Final Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "\n",
    "Unigram 分词算法是一种基于概率的分词方法，它通过最大化句子的概率来选择最佳的分词方式。与 BPE 和 WordPiece 不同，Unigram 算法不是通过合并字符对来构建词汇表，而是直接通过概率模型来选择最优的子词单元。\n",
    "\n",
    "#### Unigram 算法的基本步骤\n",
    "\n",
    "1. **初始化词汇表**：\n",
    "   - 使用所有可能的字符和子词单元初始化词汇表。\n",
    "\n",
    "2. **训练语言模型**：\n",
    "   - 使用 EM 算法（Expectation-Maximization）训练一个语言模型，计算每个子词单元的概率。\n",
    "\n",
    "3. **选择最佳分词**：\n",
    "   - 对于每个句子，选择能够最大化句子概率的分词方式。\n",
    "\n",
    "4. **更新词汇表**：\n",
    "   - 根据分词结果更新词汇表和子词单元的概率。\n",
    "\n",
    "5. **重复训练过程**：\n",
    "   - 重复步骤2到步骤4，直到词汇表和子词单元的概率收敛。\n",
    "\n",
    "#### 示例\n",
    "\n",
    "假设我们有以下文本数据：\n",
    "\n",
    "```\n",
    "\"low lower lowest\"\n",
    "```\n",
    "\n",
    "#### 初始词汇表\n",
    "\n",
    "首先，我们将每个单词分解为字符序列，并将这些字符作为初始词汇表：\n",
    "\n",
    "```\n",
    "l, o, w, e, r, s, t\n",
    "```\n",
    "\n",
    "初始词汇表为：`{'l', 'o', 'w', 'e', 'r', 's', 't'}`\n",
    "\n",
    "#### 第一次训练\n",
    "\n",
    "假设我们训练了一个语言模型，并计算了每个子词单元的概率。假设初始概率如下：\n",
    "\n",
    "```\n",
    "P(l) = 0.2, P(o) = 0.2, P(w) = 0.2, P(e) = 0.1, P(r) = 0.1, P(s) = 0.1, P(t) = 0.1\n",
    "```\n",
    "\n",
    "#### 第一次分词\n",
    "\n",
    "对于句子 `\"low\"`，我们有以下可能的分词方式：\n",
    "\n",
    "1. `l o w`：概率为 `P(l) * P(o) * P(w) = 0.2 * 0.2 * 0.2 = 0.008`\n",
    "2. `lo w`：假设 `lo` 是一个新的子词单元，概率为 `P(lo) * P(w) = 0.3 * 0.2 = 0.06`\n",
    "3. `low`：假设 `low` 是一个新的子词单元，概率为 `P(low) = 0.5`\n",
    "\n",
    "选择概率最大的分词方式 `low`。\n",
    "\n",
    "#### 更新词汇表\n",
    "\n",
    "将 `low` 加入词汇表，并更新子词单元的概率：\n",
    "\n",
    "```\n",
    "P(low) = 0.5, P(l) = 0.1, P(o) = 0.1, P(w) = 0.1, P(e) = 0.1, P(r) = 0.1, P(s) = 0.1, P(t) = 0.1\n",
    "```\n",
    "\n",
    "#### 第二次训练\n",
    "\n",
    "再次训练语言模型，并计算每个子词单元的概率。假设更新后的概率如下：\n",
    "\n",
    "```\n",
    "P(low) = 0.5, P(l) = 0.1, P(o) = 0.1, P(w) = 0.1, P(e) = 0.1, P(r) = 0.1, P(s) = 0.1, P(t) = 0.1\n",
    "```\n",
    "\n",
    "#### 第二次分词\n",
    "\n",
    "对于句子 `\"lower\"`，我们有以下可能的分词方式：\n",
    "\n",
    "1. `l o w e r`：概率为 `P(l) * P(o) * P(w) * P(e) * P(r) = 0.1 * 0.1 * 0.1 * 0.1 * 0.1 = 0.00001`\n",
    "2. `low e r`：概率为 `P(low) * P(e) * P(r) = 0.5 * 0.1 * 0.1 = 0.005`\n",
    "3. `lo w e r`：概率为 `P(lo) * P(w) * P(e) * P(r) = 0.3 * 0.1 * 0.1 * 0.1 = 0.0003`\n",
    "4. `lower`：假设 `lower` 是一个新的子词单元，概率为 `P(lower) = 0.6`\n",
    "\n",
    "选择概率最大的分词方式 `lower`。\n",
    "\n",
    "#### 更新词汇表\n",
    "\n",
    "将 `lower` 加入词汇表，并更新子词单元的概率：\n",
    "\n",
    "```\n",
    "P(lower) = 0.6, P(low) = 0.3, P(l) = 0.05, P(o) = 0.05, P(w) = 0.05, P(e) = 0.05, P(r) = 0.05, P(s) = 0.05, P(t) = 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unigram算法\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "def initialize_vocab(text):\n",
    "    # 初始化词汇表，包含所有字符\n",
    "    vocab = set()\n",
    "    for word in text.split():\n",
    "        vocab.update(list(word))\n",
    "    return vocab\n",
    "\n",
    "def compute_sentence_probability(sentence, vocab, probs):\n",
    "    # 计算句子的概率\n",
    "    probability = 1.0\n",
    "    for token in sentence.split():\n",
    "        if token in probs:\n",
    "            probability *= probs[token]\n",
    "        else:\n",
    "            probability = 0.0\n",
    "            break\n",
    "    return probability\n",
    "\n",
    "def unigram_tokenize(text, vocab, probs):\n",
    "    # 对文本进行分词\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        best_split = None\n",
    "        max_prob = 0.0\n",
    "        # 尝试所有可能的分词方式\n",
    "        for i in range(1, len(word) + 1):\n",
    "            prefix = word[:i]\n",
    "            suffix = word[i:]\n",
    "            if prefix in vocab:\n",
    "                prefix_prob = probs[prefix]\n",
    "                suffix_prob = compute_sentence_probability(suffix, vocab, probs)\n",
    "                total_prob = prefix_prob * suffix_prob\n",
    "                if total_prob > max_prob:\n",
    "                    max_prob = total_prob\n",
    "                    best_split = (prefix, suffix)\n",
    "        if best_split:\n",
    "            tokens.append(best_split[0])\n",
    "            if best_split[1]:\n",
    "                tokens.extend(unigram_tokenize(best_split[1], vocab, probs))\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def train_unigram(text, num_iterations):\n",
    "    # 初始化词汇表\n",
    "    vocab = initialize_vocab(text)\n",
    "    # 初始化子词单元的概率\n",
    "    probs = {token: 1.0 / len(vocab) for token in vocab}\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # 分词\n",
    "        tokens = unigram_tokenize(text, vocab, probs)\n",
    "        # 统计子词单元的频率\n",
    "        token_counts = Counter(tokens)\n",
    "        total_count = sum(token_counts.values())\n",
    "        # 更新子词单元的概率\n",
    "        for token in token_counts:\n",
    "            probs[token] = token_counts[token] / total_count\n",
    "        # 更新词汇表\n",
    "        vocab.update(token_counts.keys())\n",
    "    \n",
    "    return vocab, probs\n",
    "\n",
    "# 示例文本\n",
    "text = \"low lower lowest today good day is sky ok a tree new miss to\"\n",
    "\n",
    "# 设置训练迭代次数\n",
    "num_iterations = 10\n",
    "\n",
    "# 训练Unigram模型\n",
    "vocab, probs = train_unigram(text, num_iterations)\n",
    "\n",
    "# 输出最终的词汇表和子词单元的概率\n",
    "print(\"Final Vocabulary:\", vocab)\n",
    "print(\"Final Probabilities:\", probs)\n",
    "\n",
    "# 对新的句子进行分词\n",
    "new_sentence = \"Today is a good day\"\n",
    "tokens = unigram_tokenize(new_sentence, vocab, probs)\n",
    "print(\"Tokenized Sentence:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece\n",
    "\n",
    "**SentencePiece** 是一种无监督的分词算法，广泛应用于自然语言处理任务中（如机器翻译、文本生成等）。它的核心思想是将文本直接视为一个字符序列，并通过统计学习方法（如 BPE 或 Unigram）将字符序列分割成子词单元。SentencePiece 的一个重要特点是它不需要预先分词，可以直接从原始文本中学习分词模型。\n",
    "\n",
    "#### SentencePiece 的主要特点\n",
    "\n",
    "1. **无监督学习**：\n",
    "   - SentencePiece 直接从原始文本中学习分词模型，不需要预先分词。\n",
    "   \n",
    "2. **支持多种分词算法**：\n",
    "   - 支持 BPE（Byte Pair Encoding）和 Unigram 两种分词算法。\n",
    "\n",
    "3. **字符级别的处理**：\n",
    "   - 将文本视为字符序列，能够处理任意语言的文本。\n",
    "\n",
    "4. **子词单元**：\n",
    "   - 将文本分割成子词单元，能够有效处理未登录词（OOV）和稀有词汇。\n",
    "\n",
    "\n",
    "#### SentencePiece 的算法过程\n",
    "\n",
    "#### 1. 初始化\n",
    "- 将文本视为字符序列，并将每个字符作为初始词汇表。\n",
    "\n",
    "#### 2. 训练分词模型\n",
    "- 使用 BPE 或 Unigram 算法训练分词模型：\n",
    "  - **BPE**：通过合并高频字符对来构建词汇表。\n",
    "  - **Unigram**：通过最大化句子的概率来选择最佳的子词单元。\n",
    "\n",
    "#### 3. 分词\n",
    "- 使用训练好的模型将文本分割成子词单元。\n",
    "\n",
    "#### 4. 解码\n",
    "- 将子词单元转换回原始文本。\n",
    "\n",
    "SentencePiece 官方提供了 Python 库，可以直接使用。以下是使用 SentencePiece 库的示例代码：\n",
    "\n",
    "#### 安装 SentencePiece\n",
    "\n",
    "```bash\n",
    "pip install sentencepiece\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['▁', 'l', 'o', 'w', '▁', 'l', 'o', 'w', 'e', 'r', '▁', 'l', 'o', 'w', 'e', 's', 't']\n",
      "Detokenized Text: low lower lowest\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# 示例文本\n",
    "text = \"low lower lowest\"\n",
    "\n",
    "# 将文本保存到文件中\n",
    "with open(\"text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# 训练 SentencePiece 模型\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"text.txt\",  # 输入文件\n",
    "    model_prefix=\"spm_model\",  # 模型前缀\n",
    "    vocab_size=12,  # 词汇表大小\n",
    "    model_type=\"bpe\",  # 使用 BPE 算法\n",
    "    character_coverage=1.0,  # 字符覆盖率\n",
    "    pad_id=0,  # Padding ID\n",
    "    unk_id=1,  # Unknown token ID\n",
    "    bos_id=2,  # Beginning of sentence ID\n",
    "    eos_id=3  # End of sentence ID\n",
    ")\n",
    "\n",
    "# 加载训练好的模型\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_model.model\")\n",
    "\n",
    "# 对文本进行分词\n",
    "tokens = sp.encode_as_pieces(\"low lower lowest\")\n",
    "print(\"Tokenized Sentence:\", tokens)\n",
    "\n",
    "# 将子词单元转换回文本\n",
    "detokenized_text = sp.decode_pieces(tokens)\n",
    "print(\"Detokenized Text:\", detokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 典型大模型分词器\n",
    "\n",
    "|算法|模型|\n",
    "|----|----|\n",
    "|BPE       |GPT, GPT-2, GPT-J, GPT-Neo, RoBERTa, BART, LLaMA, ChatGLM-6B, Baichuan||\n",
    "|WordPiece |BERT, DistilBERT，MobileBERT||\n",
    "|Unigram   |AlBERT, T5, mBART, XLNet|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face分词器\n",
    "\n",
    "Hugging Face 的 `tokenizers` 库是一个强大的工具，支持多种分词算法。以下是 `tokenizers` 库支持的主要分词算法及其特点：\n",
    "\n",
    "1. **BPE (Byte Pair Encoding)**\n",
    "   - **特点**:\n",
    "     - 通过迭代地合并最频繁出现的字符对来构建词汇表。\n",
    "     - 能够处理未登录词（OOV），因为它可以将未知单词分解为已知的子词单元。\n",
    "     - 适用于多种语言，包括低资源语言。\n",
    "   - **使用场景**:\n",
    "     - 许多现代NLP模型如OpenAI的GPT系列、Facebook的MarianMT等都使用BPE。\n",
    "\n",
    "2. **WordPiece**\n",
    "   - **特点**:\n",
    "     - 结合了字节对编码的优点，并保留了一些完整的单词。\n",
    "     - 使用贪婪算法来构建词汇表。\n",
    "     - 在保持较小词汇表的同时，尽量保留完整的单词。\n",
    "   - **使用场景**:\n",
    "     - Google的BERT模型使用WordPiece作为默认的分词器。\n",
    "     - 微软的RoBERTa也使用WordPiece。\n",
    "\n",
    "3. **SentencePiece**\n",
    "   - **特点**:\n",
    "     - 支持多种训练算法，包括Unigram、BPE、Char and Word N-gram等。\n",
    "     - 提供灵活的配置选项，可以根据具体需求选择不同的算法。\n",
    "     - 高效且易于集成。\n",
    "   - **使用场景**:\n",
    "     - Google的T5、Facebook的XLM-R等模型使用SentencePiece。\n",
    "\n",
    "4. **Unigram**\n",
    "   - **特点**:\n",
    "     - 基于统计的方法，根据概率分布选择最优的分割方式。\n",
    "     - 可以生成更平衡的词汇表。\n",
    "   - **使用场景**:\n",
    "     - 适用于需要平衡词汇表的应用。\n",
    "\n",
    "5. **Metaspace**\n",
    "   - **特点**:\n",
    "     - 基于空格和特殊标记的分词方法。\n",
    "     - 适用于需要简单高效的分词任务。\n",
    "   - **使用场景**:\n",
    "     - 一些特定的任务或研究项目。\n",
    "\n",
    "6. **Char-level Tokenization**\n",
    "   - **特点**:\n",
    "     - 将文本分割成单个字符。\n",
    "     - 适合处理低资源语言或需要精细粒度的场景。\n",
    "   - **使用场景**:\n",
    "     - 某些特定任务或研究项目可能会使用字符级别的分词。\n",
    "\n",
    "7. **Whitespace Tokenization**\n",
    "   - **特点**:\n",
    "     - 简单地按照空格将文本分割成单词。\n",
    "     - 实现简单，但不能处理复合词或未登录词。\n",
    "   - **使用场景**:\n",
    "     - 基础NLP任务或原型系统开发。\n",
    "\n",
    "8. **Pretokenized Input**\n",
    "   - **特点**:\n",
    "     - 直接接受预分好的单词列表。\n",
    "     - 适用于已经进行过分词的数据集。\n",
    "   - **使用场景**:\n",
    "     - 数据集已经经过预处理的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE 分词器\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, normalizers\n",
    "\n",
    "# 创建BPE模型\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# 添加预处理器\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 训练BPE模型\n",
    "files = [\"path/to/train.txt\"]\n",
    "tokenizer.train(files)\n",
    "\n",
    "# 保存模型\n",
    "tokenizer.save(\"bpe-tokenizer.json\")\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer.json\")\n",
    "\n",
    "# 对文本进行分词\n",
    "text = \"Hello, how are you doing today?\"\n",
    "output = tokenizer.encode(text)\n",
    "print(\"Tokens:\", output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece 分词器\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, normalizers\n",
    "\n",
    "# 创建WordPiece模型\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 添加预处理器\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 训练WordPiece模型\n",
    "files = [\"path/to/train.txt\"]\n",
    "tokenizer.train(files)\n",
    "\n",
    "# 保存模型\n",
    "tokenizer.save(\"wordpiece-tokenizer.json\")\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = Tokenizer.from_file(\"wordpiece-tokenizer.json\")\n",
    "\n",
    "# 对文本进行分词\n",
    "text = \"Hello, how are you doing today?\"\n",
    "output = tokenizer.encode(text)\n",
    "print(\"Tokens:\", output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SentencePiece 分词器\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, normalizers\n",
    "\n",
    "# 创建SentencePiece模型\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "# 添加预处理器\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 训练SentencePiece模型\n",
    "files = [\"train.txt\"]\n",
    "tokenizer.train(files)\n",
    "\n",
    "# 保存模型\n",
    "tokenizer.save(\"sentencepiece-tokenizer.json\")\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = Tokenizer.from_file(\"sentencepiece-tokenizer.json\")\n",
    "\n",
    "# 对文本进行分词\n",
    "text = \"Hello, how are you doing today?\"\n",
    "output = tokenizer.encode(text)\n",
    "print(\"Tokens:\", output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tiktoken**\n",
    "\n",
    "**Tiktoken** 是 OpenAI 开发的一个高效的分词工具，专门用于 GPT 系列模型（如 GPT-3、GPT-4）的分词任务。它基于字节对编码（BPE，Byte Pair Encoding）算法，能够将文本快速转换为模型所需的 token ID 序列，同时支持将 token ID 序列转换回原始文本。\n",
    "\n",
    "Tiktoken 的主要特点包括：\n",
    "1. **高效性**：针对 GPT 系列模型优化，分词速度极快。\n",
    "2. **多语言支持**：能够处理多种语言的文本。\n",
    "3. **可扩展性**：支持自定义词汇表和分词规则。\n",
    "4. **与 GPT 模型兼容**：直接与 OpenAI 的 GPT 系列模型集成。\n",
    "\n",
    "Tiktoken 的核心功能包括：\n",
    "- **编码（Tokenization）**：将文本转换为 token ID 序列。\n",
    "- **解码（Detokenization）**：将 token ID 序列转换回文本。\n",
    "\n",
    "### **Tiktoken 的使用场景**\n",
    "\n",
    "1. **文本预处理**：\n",
    "   - 在将文本输入 GPT 模型之前，需要将文本转换为 token ID 序列。\n",
    "2. **文本生成**：\n",
    "   - 在 GPT 模型生成文本后，需要将 token ID 序列转换回可读的文本。\n",
    "3. **计算 token 数量**：\n",
    "   - 统计文本的 token 数量，用于控制输入长度或计算成本。\n",
    "\n",
    "### **安装 Tiktoken**\n",
    "```\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 加载分词器\n",
    "model_name = \"bert/bert-base-uncased\"  # 选择合适的模型名称\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# 示例文本\n",
    "text = \"Hello, how are you doing today?\"\n",
    "\n",
    "# 编码：将文本转换为 token ID 序列\n",
    "token_ids = encoder.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 解码：将 token ID 序列转换回文本\n",
    "decoded_text = encoder.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "# 统计 token 数量\n",
    "token_count = len(token_ids)\n",
    "print(\"Token Count:\", token_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evn_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
