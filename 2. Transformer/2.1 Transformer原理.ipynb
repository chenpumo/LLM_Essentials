{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Transformer原理\n",
    "Transformer 是一种基于自注意力机制的神经网络架构，由谷歌于2017年在论文[《Attention Is All You Need》](https://arxiv.org/pdf/1706.03762)中提出。Transformer模型摒弃了固有的定式，并没有用任何CNN或者RNN的结构，而是使用了Attention注意力机制，自动捕捉输入序列不同位置处的相对关联，善于处理较长文本，并且该模型可以高度并行地工作，训练速度很快。\n",
    "\n",
    "### **Transformer整体结构**\n",
    "\n",
    "<center>\n",
    "<img src='./images/transformer.png' height='600'/>\n",
    "\n",
    "Transformer模型结构图\n",
    "</center>\n",
    "\n",
    "Transformer 由编码器和解码器组成，每层包含以下模块：\n",
    "- **编码器层**：\n",
    "  1. 多头自注意力机制 + 残差连接 + 归一化。\n",
    "  2. 前馈全连接层 + 残差连接 + 归一化。\n",
    "- **解码器层**：\n",
    "  1. 掩码多头自注意力机制 + 残差连接 + 归一化。\n",
    "  2. 多头注意力机制（编码器-解码器注意力） + 残差连接 + 归一化。\n",
    "  3. 前馈全连接层 + 残差连接 + 归一化。\n",
    "\n",
    "#### **Embedding 层**\n",
    "Embedding 层将输入文本中的单词转换成向量表示，即词嵌入（word embeddings）。这些向量捕捉了词汇表中每个单词的语义信息。在Transformer中，通常使用预训练好的词向量，如Word2Vec或GloVe，或者是在训练过程中学习得到的词嵌入。\n",
    "\n",
    "- **输入**：一个符号序列（如单词序列），每个符号用整数索引表示。\n",
    "- **输出**：每个符号对应的嵌入向量。\n",
    "\n",
    "假设词汇表大小为 $V$，嵌入维度为 $d_{\\text{model}}$，则嵌入矩阵 $E \\in \\mathbb{R}^{V \\times d_{\\text{model}}}$ 将符号索引 $i$ 映射为向量：\n",
    "$$\n",
    "\\text{Embedding}(i) = E_i\n",
    "$$\n",
    "\n",
    "#### **位置编码（Positional Encoding）**\n",
    "由于Transformer不使用RNN或CNN，它没有显式的序列顺序信息，因此需要通过位置编码为输入序列添加位置信息。\n",
    "\n",
    "- **输入**：嵌入向量序列。\n",
    "- **输出**：带有位置信息的嵌入向量序列。\n",
    "\n",
    "位置编码利用正弦和余弦函数的不同频率为序列中的每个位置生成唯一的编码：\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "其中，$pos$ 是位置索引，$i$ 是维度索引，$d_\\text{model}$ 是维度索引。将位置编码加到嵌入向量上：\n",
    "$$\n",
    "X = \\text{Embedding} + PE\n",
    "$$\n",
    "\n",
    "假设有一个简单的中文句子：“你好吗”。将这个句子转换为词嵌入，并添加位置编码。设定`d_model`（模型的维度）为512，并且每个词已经被转换成了512维的词嵌入向量。\n",
    "\n",
    "对于第一个词“你”，它的位置是0，所以：\n",
    "\n",
    "- $PE_{(0, 0)} = \\sin\\left(\\frac{0}{10000^{0/8}}\\right)$，$PE_{(0, 1)} = \\cos\\left(\\frac{0}{10000^{0/8}}\\right)$，$PE_{(0, 2)} = \\sin\\left(\\frac{0}{10000^{2/8}}\\right)$，$PE_{(0, 3)} = \\cos\\left(\\frac{0}{10000^{2/8}}\\right)$，...\n",
    "\n",
    "- 以此类推，直到填满整个512维的位置编码向量。\n",
    "\n",
    "对于第二个词“好”，它的位置是1，所以：\n",
    "- $PE_{(1, 0)} = \\sin\\left(\\frac{1}{10000^{0/8}}\\right)$，$PE_{(1, 1)} = \\cos\\left(\\frac{1}{10000^{0/8}}\\right)$，$PE_{(1, 2)} = \\sin\\left(\\frac{1}{10000^{2/8}}\\right)$，...\n",
    "- 以此类推，直到填满整个512维的位置编码向量。\n",
    "\n",
    "对于第三个词“吗”，它的位置是2，那么：\n",
    "- $PE_{(2, 0)} = \\sin\\left(\\frac{2}{10000^{0/512}}\\right)$,$PE_{(2, 1)} = \\cos\\left(\\frac{2}{10000^{0/512}}\\right)$，$PE_{(2, 2)} = \\sin\\left(\\frac{2}{10000^{2/8}}\\right)$，...\n",
    "\n",
    "\n",
    "#### **注意力机制（Attention）**\n",
    "假设要将英文句子 \"The cat sat on the mat.\" 翻译成法语 \"Le chat était assis sur le tapis.\" 我们可以使用基于RNN（循环神经网络）的序列到序列（Seq2Seq）模型来进行这个翻译任务。然而，传统的Seq2Seq模型有一个限制，即编码器（Encoder）需要将整个源句子的信息压缩成一个固定长度的向量表示，然后解码器（Decoder）根据这个单一的向量生成目标语言的句子。这种方法对于较长的句子来说效果并不理想，因为过多的信息被压缩进了一个向量，可能会导致信息丢失。\n",
    "\n",
    "为了解决这个问题，Bahdanau等人在2015年提出了注意力机制。\n",
    "\n",
    "**缩放点积注意力（Scaled Dot-Product Attention）**\n",
    "- **输入**：查询矩阵 $Q$、键矩阵 $K$、值矩阵 $V$。\n",
    "- **输出**：加权后的值向量。\n",
    "\n",
    "计算注意力分数：\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "其中，$d_k$ 是键向量的维度，$\\sqrt{d_k}$ 用于缩放点积，防止梯度消失。\n",
    "\n",
    "\n",
    "为了将注意力机制的计算过程用向量或矩阵表示，我们可以以一个简化版的例子来说明。假设我们有一个简单的输入序列，每个词被嵌入到一个二维空间中（为了便于演示，实际应用中维度通常会更大）。我们的目标是通过注意力机制计算出针对每个词的上下文向量。\n",
    "\n",
    "**Attention计算过程示例**\n",
    "\n",
    "假设有三个词的嵌入向量组成的输入矩阵 $X$，每个词的嵌入向量为2维：\n",
    "\n",
    "$$ X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} $$\n",
    "\n",
    "**（1）计算Q, K, V**\n",
    "\n",
    "使用不同的权重矩阵 $W^Q$, $W^K$, 和 $W^V$ 来计算查询（Query）、键（Key）和值（Value）向量。为了简化计算，假定这些权重矩阵也是2维的。\n",
    "\n",
    "$$ W^Q = \\begin{bmatrix} 0.5 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix}, \\quad W^K = \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.6 & 0.5 \\end{bmatrix}, \\quad W^V = \\begin{bmatrix} 0.2 & 0.3 \\\\ 0.4 & 0.5 \\end{bmatrix} $$\n",
    "\n",
    "通过矩阵乘法得到 Q, K, V：\n",
    "\n",
    "$$ Q = XW^Q = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0.5 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix} = \\begin{bmatrix} 2.1 & 2.5 \\\\ 4.7 & 5.7 \\\\ 7.3 & 8.9 \\end{bmatrix} $$\n",
    "\n",
    "$$ K = XW^K = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.6 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 & 1.4 \\\\ 3.3 & 3.2 \\\\ 5.1 & 5.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ V = XW^V = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0.2 & 0.3 \\\\ 0.4 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 1.0 & 1.3 \\\\ 2.2 & 2.9 \\\\ 3.4 & 4.5 \\end{bmatrix} $$\n",
    "**（2）计算注意力分数**\n",
    "\n",
    "计算点积相似度并除以键向量维度的平方根（即 $\\sqrt{d_k}$），其中 $d_k=2$：\n",
    "\n",
    "$$ S = \\frac{QK^T}{\\sqrt{d_k}} $$\n",
    "\n",
    "$$ S = \\frac{\\begin{bmatrix} 2.1 & 2.5 \\\\ 4.7 & 5.7 \\\\ 7.3 & 8.9 \\end{bmatrix} \\begin{bmatrix} 1.5 & 3.3 & 5.1 \\\\ 1.4 & 3.2 & 5.0 \\end{bmatrix}}{\\sqrt{2}} $$\n",
    "\n",
    "$$ S = \\frac{\\begin{bmatrix} 7.7 & 16.5 & 25.3 \\\\ 17.49 & 36.3 & 55.11 \\\\ 27.28 & 56.1 & 84.92 \\end{bmatrix}}{\\sqrt{2}} \\approx \\begin{bmatrix} 5.44 & 11.67 & 17.90 \\\\ 12.37 & 25.69 & 39.00 \\\\ 19.28 & 39.71 & 60.11 \\end{bmatrix} $$\n",
    "\n",
    "**（3）归一化**\n",
    "\n",
    "对得分进行softmax归一化处理，得到注意力权重矩阵 $A$：\n",
    "\n",
    "$$ A = \\text{softmax}(S) $$\n",
    "\n",
    "**（4）上下文向量**\n",
    "\n",
    "最后，利用注意力权重对值矩阵加权求和，得到上下文向量 $C$：\n",
    "\n",
    "$$ C = AV $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " tensor([[8.1903e-06, 2.8578e-03, 9.9713e-01],\n",
      "        [3.1802e-12, 1.7833e-06, 1.0000e+00],\n",
      "        [1.2313e-18, 1.1096e-09, 1.0000e+00]])\n",
      "Context Vector:\n",
      " tensor([[3.3966, 4.4954],\n",
      "        [3.4000, 4.5000],\n",
      "        [3.4000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设的输入、权重矩阵\n",
    "X = torch.tensor([[1., 2.], [3., 4.], [5., 6.]])\n",
    "W_Q = torch.tensor([[0.5, 0.7], [0.8, 0.9]])\n",
    "W_K = torch.tensor([[0.3, 0.4], [0.6, 0.5]])\n",
    "W_V = torch.tensor([[0.2, 0.3], [0.4, 0.5]])\n",
    "\n",
    "# 计算Q, K, V\n",
    "Q = torch.matmul(X, W_Q)\n",
    "K = torch.matmul(X, W_K)\n",
    "V = torch.matmul(X, W_V)\n",
    "\n",
    "# 计算注意力分数并归一化\n",
    "scores = torch.matmul(Q, K.t()) / (K.size(1)**0.5)\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# 计算上下文向量\n",
    "context_vector = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Context Vector:\\n\", context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **多头注意力机制（Multi-Head Attention）**\n",
    "多头注意力通过并行计算多个注意力头，捕捉不同的子空间信息。\n",
    "\n",
    "- **输入**：查询矩阵 $Q$、键矩阵 $K$、值矩阵 $V$。\n",
    "- **输出**：多个注意力头的拼接结果。\n",
    "\n",
    "- **线性变换**：\n",
    "   对于输入序列 $X$，先通过线性变换生成 $Q$、$K$ 和 $V$ 矩阵。\n",
    "   \n",
    "   $$ Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V $$\n",
    "\n",
    "\n",
    "- **分割成多个头**：\n",
    "   将 $Q$、$K$ 和 $V$ 按照指定的头数 $h$ 切分成更小的矩阵，使得每个头都有自己的 $Q_i$、$K_i$ 和 $V_i$ 矩阵，它们的维度通常为 $d_{model}/h$。\n",
    "\n",
    "- **计算每个头的注意力输出**：\n",
    "   对于每个头 $i$，使用上述公式计算其注意力输出 $head_i$。\n",
    "   $$\n",
    "   \\text{head}_h = \\text{Attention}(QW_h^Q, KW_h^K, VW_h^V)\n",
    "   $$\n",
    "\n",
    "- **合并与最终变换**：\n",
    "   将所有头的输出拼接起来形成一个新的矩阵，并通过另一个线性变换将其映射回原始维度，即：\n",
    "\n",
    "   $$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O $$\n",
    "\n",
    "   其中 $W^O$ 是最终线性变换的权重矩阵。\n",
    "\n",
    "\n",
    "#### **残差连接与归一化层**\n",
    "残差连接（Residual Connection）用于缓解梯度消失问题，帮助模型训练更深层的网络。\n",
    "\n",
    "- **输入**：子层（如多头注意力或 FFN）的输入和输出。\n",
    "- **输出**：输入与输出的和。\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Layer}(x) + x\n",
    "$$\n",
    "\n",
    "归一化层（Layer Normalization）对每个样本的特征进行归一化，加速训练并提高稳定性。\n",
    "\n",
    "- **输入**：子层的输出。\n",
    "- **输出**：归一化后的结果。\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "其中，$\\mu$ 和 $\\sigma^2$ 是均值和方差，$\\gamma$ 和 $\\beta$ 是可学习的参数，$\\epsilon$ 是防止除零的小常数。\n",
    "\n",
    "#### **前馈全连接层（Feed-Forward Network, FFN）**\n",
    "前馈全连接层对每个位置的表示进行非线性变换。\n",
    "\n",
    "- **输入**：多头注意力层的输出。\n",
    "- **输出**：经过非线性变换的表示。\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "其中，$W_1, W_2$ 是权重矩阵，$b_1, b_2$ 是偏置项。\n",
    "\n",
    "#### **Softmax 和输出层**\n",
    "Softmax 用于将模型的输出转换为概率分布。\n",
    "\n",
    "- **输入**：解码器的最终输出。\n",
    "- **输出**：目标词汇的概率分布。\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^V e^{z_j}}\n",
    "$$\n",
    "其中，$z_i$ 是第 $i$ 个词汇的得分，$V$ 是词汇表大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 示例：Bert模型\n",
    "### Bert\n",
    "\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer架构的预训练语言模型，由Google在2018年提出。BERT的核心思想是通过双向上下文来理解文本，而不是像传统的语言模型那样只从左到右或从右到左进行建模。参见[BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n",
    "\n",
    "#### BERT的主要结构\n",
    "1. **输入表示**：\n",
    "   - **Token Embeddings**：将输入的单词或子词转换为向量表示。\n",
    "   - **Segment Embeddings**：用于区分句子对（例如，问答任务中的问题和答案）。如果输入包含两个句子，则需要区分这两个句子。为此，BERT为第一个句子的所有tokens分配一个值0，第二个句子的所有tokens分配一个值1。\n",
    "   - **Position Embeddings**：表示单词在句子中的位置信息。\n",
    "\n",
    "2. **Transformer Encoder**：\n",
    "   - BERT使用了多层Transformer Encoder堆叠而成。\n",
    "   - 每个Transformer Encoder层包含两个子层：\n",
    "     - **Multi-Head Self-Attention**：计算输入序列中每个单词与其他单词的相关性。\n",
    "     - **Feed-Forward Neural Network**：对每个位置的表示进行非线性变换。\n",
    "\n",
    "3. **输出**：\n",
    "   - BERT的输出是每个输入token的上下文表示，可以用于各种下游任务（如文本分类、命名实体识别等）。\n",
    "\n",
    "  ![alt text](images/Bert.jpg)\n",
    "\n",
    "#### BERT的预训练任务\n",
    "\n",
    "1. **Masked Language Model (MLM)**：\n",
    "   - 随机掩盖输入序列中的一些token，然后让模型预测这些被掩盖的token。\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**：\n",
    "   - 给定两个句子，模型预测第二个句子是否是第一个句子的下一句。\n",
    "\n",
    "#### 实现一个基于BERT的文本分类器\n",
    "\n",
    "下面是一个使用`transformers`库和`PyTorch`实现基于BERT的文本分类器的示例代码，参见源码bert_cls.py。\n",
    "\n",
    "1. **数据准备**：\n",
    "   - 使用`train_test_split`将数据划分为训练集和测试集。\n",
    "   - 自定义`TextDataset`类来处理文本数据，并将其转换为BERT模型所需的输入格式。\n",
    "\n",
    "2. **模型加载**：\n",
    "   - 使用`BertForSequenceClassification`加载预训练的BERT模型，并指定分类的类别数（`num_labels=2`表示二分类）。\n",
    "\n",
    "3. **训练和评估**：\n",
    "   - 定义`train_epoch`和`eval_model`函数来分别进行训练和评估。\n",
    "   - 使用`AdamW`优化器进行模型训练。\n",
    "\n",
    "4. **设备设置**：\n",
    "   - 检查是否有可用的GPU，并将模型和数据移动到相应的设备上。\n",
    "\n",
    "5. **训练循环**：\n",
    "   - 进行多个epoch的训练，并在每个epoch结束后评估模型在验证集上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 示例数据\n",
    "'''\n",
    "texts = [\"I love programming\", \"I hate bugs\", \"Python is great\", \"Debugging is frustrating\"]\n",
    "labels = [1, 0, 1, 0]  # 1: Positive, 0: Negative\n",
    "# 划分训练集和测试集\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "'''\n",
    "import pandas as pd\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love programming in Python!',\n",
    "        'This is the worst experience I have ever had.',\n",
    "        'Absolutely fantastic! Highly recommend it.',\n",
    "        'Not good, not bad, just okay.',\n",
    "        'The movie was thrilling and full of action.',\n",
    "        'I did not like the plot of the book.',\n",
    "        'The concert was amazing with great music and energy.',\n",
    "        'The food at the restaurant was terrible.'\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1, 2, 1, 2, 1]  # 假设有3个类别：0 - 正面，1 - 负面，2 - 中性\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"bert/bert-base-uncased\"\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 自定义Dataset类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 创建Dataset和DataLoader\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 训练函数\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n",
    "\n",
    "# 评估函数\n",
    "def eval_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 记录损失\n",
    "loss_history = []\n",
    "# 训练和评估\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    loss_history.append(train_loss)\n",
    "    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, test_loader, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')\n",
    "\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(loss_history)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
