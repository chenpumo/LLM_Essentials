{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Transformer原理\n",
    "Transformer 是一种基于自注意力机制的神经网络架构，由谷歌于2017年在论文《Attention Is All You Need》中提出。Transformer模型摒弃了固有的定式，并没有用任何CNN或者RNN的结构，而是使用了Attention注意力机制，自动捕捉输入序列不同位置处的相对关联，善于处理较长文本，并且该模型可以高度并行地工作，训练速度很快。\n",
    "\n",
    "### **Transformer整体结构**\n",
    "\n",
    "<center>\n",
    "<img src='./images/transformer.png' height='600'/>\n",
    "\n",
    "Transformer模型结构图\n",
    "</center>\n",
    "\n",
    "Transformer 由编码器和解码器组成，每层包含以下模块：\n",
    "- **编码器层**：\n",
    "  1. 多头自注意力机制 + 残差连接 + 归一化。\n",
    "  2. 前馈全连接层 + 残差连接 + 归一化。\n",
    "- **解码器层**：\n",
    "  1. 掩码多头自注意力机制 + 残差连接 + 归一化。\n",
    "  2. 多头注意力机制（编码器-解码器注意力） + 残差连接 + 归一化。\n",
    "  3. 前馈全连接层 + 残差连接 + 归一化。\n",
    "\n",
    "#### **Embedding 层**\n",
    "Embedding 层将输入文本中的单词转换成向量表示，即词嵌入（word embeddings）。这些向量捕捉了词汇表中每个单词的语义信息。在Transformer中，通常使用预训练好的词向量，如Word2Vec或GloVe，或者是在训练过程中学习得到的词嵌入。\n",
    "\n",
    "- **输入**：一个符号序列（如单词序列），每个符号用整数索引表示。\n",
    "- **输出**：每个符号对应的嵌入向量。\n",
    "\n",
    "假设词汇表大小为 $V$，嵌入维度为 $d_{\\text{model}}$，则嵌入矩阵 $E \\in \\mathbb{R}^{V \\times d_{\\text{model}}}$ 将符号索引 $i$ 映射为向量：\n",
    "$$\n",
    "\\text{Embedding}(i) = E_i\n",
    "$$\n",
    "\n",
    "#### **位置编码（Positional Encoding）**\n",
    "由于Transformer不使用RNN或CNN，它没有显式的序列顺序信息，因此需要通过位置编码为输入序列添加位置信息。\n",
    "\n",
    "- **输入**：嵌入向量序列。\n",
    "- **输出**：带有位置信息的嵌入向量序列。\n",
    "\n",
    "位置编码利用正弦和余弦函数的不同频率为序列中的每个位置生成唯一的编码：\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "其中，$pos$ 是位置索引，$i$ 是维度索引，$d_\\text{model}$ 是维度索引。将位置编码加到嵌入向量上：\n",
    "$$\n",
    "X = \\text{Embedding} + PE\n",
    "$$\n",
    "\n",
    "假设有一个简单的中文句子：“你好吗”。将这个句子转换为词嵌入，并添加位置编码。设定`d_model`（模型的维度）为512，并且每个词已经被转换成了512维的词嵌入向量。\n",
    "\n",
    "对于第一个词“你”，它的位置是0，所以：\n",
    "\n",
    "- $PE_{(0, 0)} = \\sin\\left(\\frac{0}{10000^{0/8}}\\right)$，$PE_{(0, 1)} = \\cos\\left(\\frac{0}{10000^{0/8}}\\right)$，$PE_{(0, 2)} = \\sin\\left(\\frac{0}{10000^{2/8}}\\right)$，$PE_{(0, 3)} = \\cos\\left(\\frac{0}{10000^{2/8}}\\right)$，...\n",
    "\n",
    "- 以此类推，直到填满整个512维的位置编码向量。\n",
    "\n",
    "对于第二个词“好”，它的位置是1，所以：\n",
    "- $PE_{(1, 0)} = \\sin\\left(\\frac{1}{10000^{0/8}}\\right)$，$PE_{(1, 1)} = \\cos\\left(\\frac{1}{10000^{0/8}}\\right)$，$PE_{(1, 2)} = \\sin\\left(\\frac{1}{10000^{2/8}}\\right)$，...\n",
    "- 以此类推，直到填满整个512维的位置编码向量。\n",
    "\n",
    "对于第三个词“吗”，它的位置是2，那么：\n",
    "- $PE_{(2, 0)} = \\sin\\left(\\frac{2}{10000^{0/512}}\\right)$,$PE_{(2, 1)} = \\cos\\left(\\frac{2}{10000^{0/512}}\\right)$，$PE_{(2, 2)} = \\sin\\left(\\frac{2}{10000^{2/8}}\\right)$，...\n",
    "\n",
    "\n",
    "#### **注意力机制（Attention）**\n",
    "假设要将英文句子 \"The cat sat on the mat.\" 翻译成法语 \"Le chat était assis sur le tapis.\" 我们可以使用基于RNN（循环神经网络）的序列到序列（Seq2Seq）模型来进行这个翻译任务。然而，传统的Seq2Seq模型有一个限制，即编码器（Encoder）需要将整个源句子的信息压缩成一个固定长度的向量表示，然后解码器（Decoder）根据这个单一的向量生成目标语言的句子。这种方法对于较长的句子来说效果并不理想，因为过多的信息被压缩进了一个向量，可能会导致信息丢失。\n",
    "\n",
    "为了解决这个问题，Bahdanau等人在2015年提出了注意力机制。\n",
    "\n",
    "**缩放点积注意力（Scaled Dot-Product Attention）**\n",
    "- **输入**：查询矩阵 $Q$、键矩阵 $K$、值矩阵 $V$。\n",
    "- **输出**：加权后的值向量。\n",
    "\n",
    "计算注意力分数：\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "其中，$d_k$ 是键向量的维度，$\\sqrt{d_k}$ 用于缩放点积，防止梯度消失。\n",
    "\n",
    "\n",
    "为了将注意力机制的计算过程用向量或矩阵表示，我们可以以一个简化版的例子来说明。假设我们有一个简单的输入序列，每个词被嵌入到一个二维空间中（为了便于演示，实际应用中维度通常会更大）。我们的目标是通过注意力机制计算出针对每个词的上下文向量。\n",
    "\n",
    "**Attention计算过程示例**\n",
    "\n",
    "假设有三个词的嵌入向量组成的输入矩阵 $X$，每个词的嵌入向量为2维：\n",
    "\n",
    "$$ X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} $$\n",
    "\n",
    "**（1）计算Q, K, V**\n",
    "\n",
    "使用不同的权重矩阵 $W^Q$, $W^K$, 和 $W^V$ 来计算查询（Query）、键（Key）和值（Value）向量。为了简化计算，假定这些权重矩阵也是2维的。\n",
    "\n",
    "$$ W^Q = \\begin{bmatrix} 0.5 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix}, \\quad W^K = \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.6 & 0.5 \\end{bmatrix}, \\quad W^V = \\begin{bmatrix} 0.2 & 0.3 \\\\ 0.4 & 0.5 \\end{bmatrix} $$\n",
    "\n",
    "通过矩阵乘法得到 Q, K, V：\n",
    "\n",
    "$$ Q = XW^Q = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0.5 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix} = \\begin{bmatrix} 2.1 & 2.5 \\\\ 4.7 & 5.7 \\\\ 7.3 & 8.9 \\end{bmatrix} $$\n",
    "\n",
    "$$ K = XW^K = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.6 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 & 1.4 \\\\ 3.3 & 3.2 \\\\ 5.1 & 5.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ V = XW^V = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0.2 & 0.3 \\\\ 0.4 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 1.0 & 1.3 \\\\ 2.2 & 2.9 \\\\ 3.4 & 4.5 \\end{bmatrix} $$\n",
    "**（2）计算注意力分数**\n",
    "\n",
    "计算点积相似度并除以键向量维度的平方根（即 $\\sqrt{d_k}$），其中 $d_k=2$：\n",
    "\n",
    "$$ S = \\frac{QK^T}{\\sqrt{d_k}} $$\n",
    "\n",
    "$$ S = \\frac{\\begin{bmatrix} 2.1 & 2.5 \\\\ 4.7 & 5.7 \\\\ 7.3 & 8.9 \\end{bmatrix} \\begin{bmatrix} 1.5 & 3.3 & 5.1 \\\\ 1.4 & 3.2 & 5.0 \\end{bmatrix}}{\\sqrt{2}} $$\n",
    "\n",
    "$$ S = \\frac{\\begin{bmatrix} 7.7 & 16.5 & 25.3 \\\\ 17.49 & 36.3 & 55.11 \\\\ 27.28 & 56.1 & 84.92 \\end{bmatrix}}{\\sqrt{2}} \\approx \\begin{bmatrix} 5.44 & 11.67 & 17.90 \\\\ 12.37 & 25.69 & 39.00 \\\\ 19.28 & 39.71 & 60.11 \\end{bmatrix} $$\n",
    "\n",
    "**（3）归一化**\n",
    "\n",
    "对得分进行softmax归一化处理，得到注意力权重矩阵 $A$：\n",
    "\n",
    "$$ A = \\text{softmax}(S) $$\n",
    "\n",
    "**（4）上下文向量**\n",
    "\n",
    "最后，利用注意力权重对值矩阵加权求和，得到上下文向量 $C$：\n",
    "\n",
    "$$ C = AV $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " tensor([[8.1903e-06, 2.8578e-03, 9.9713e-01],\n",
      "        [3.1802e-12, 1.7833e-06, 1.0000e+00],\n",
      "        [1.2313e-18, 1.1096e-09, 1.0000e+00]])\n",
      "Context Vector:\n",
      " tensor([[3.3966, 4.4954],\n",
      "        [3.4000, 4.5000],\n",
      "        [3.4000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设的输入、权重矩阵\n",
    "X = torch.tensor([[1., 2.], [3., 4.], [5., 6.]])\n",
    "W_Q = torch.tensor([[0.5, 0.7], [0.8, 0.9]])\n",
    "W_K = torch.tensor([[0.3, 0.4], [0.6, 0.5]])\n",
    "W_V = torch.tensor([[0.2, 0.3], [0.4, 0.5]])\n",
    "\n",
    "# 计算Q, K, V\n",
    "Q = torch.matmul(X, W_Q)\n",
    "K = torch.matmul(X, W_K)\n",
    "V = torch.matmul(X, W_V)\n",
    "\n",
    "# 计算注意力分数并归一化\n",
    "scores = torch.matmul(Q, K.t()) / (K.size(1)**0.5)\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# 计算上下文向量\n",
    "context_vector = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Context Vector:\\n\", context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **多头注意力机制（Multi-Head Attention）**\n",
    "多头注意力通过并行计算多个注意力头，捕捉不同的子空间信息。\n",
    "\n",
    "- **输入**：查询矩阵 $Q$、键矩阵 $K$、值矩阵 $V$。\n",
    "- **输出**：多个注意力头的拼接结果。\n",
    "\n",
    "- **线性变换**：\n",
    "   对于输入序列 $X$，先通过线性变换生成 $Q$、$K$ 和 $V$ 矩阵。\n",
    "   \n",
    "   $$ Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V $$\n",
    "\n",
    "\n",
    "- **分割成多个头**：\n",
    "   将 $Q$、$K$ 和 $V$ 按照指定的头数 $h$ 切分成更小的矩阵，使得每个头都有自己的 $Q_i$、$K_i$ 和 $V_i$ 矩阵，它们的维度通常为 $d_{model}/h$。\n",
    "\n",
    "- **计算每个头的注意力输出**：\n",
    "   对于每个头 $i$，使用上述公式计算其注意力输出 $head_i$。\n",
    "   $$\n",
    "   \\text{head}_h = \\text{Attention}(QW_h^Q, KW_h^K, VW_h^V)\n",
    "   $$\n",
    "\n",
    "- **合并与最终变换**：\n",
    "   将所有头的输出拼接起来形成一个新的矩阵，并通过另一个线性变换将其映射回原始维度，即：\n",
    "\n",
    "   $$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O $$\n",
    "\n",
    "   其中 $W^O$ 是最终线性变换的权重矩阵。\n",
    "\n",
    "\n",
    "#### **残差连接与归一化层**\n",
    "残差连接（Residual Connection）用于缓解梯度消失问题，帮助模型训练更深层的网络。\n",
    "\n",
    "- **输入**：子层（如多头注意力或 FFN）的输入和输出。\n",
    "- **输出**：输入与输出的和。\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Layer}(x) + x\n",
    "$$\n",
    "\n",
    "归一化层（Layer Normalization）对每个样本的特征进行归一化，加速训练并提高稳定性。\n",
    "\n",
    "- **输入**：子层的输出。\n",
    "- **输出**：归一化后的结果。\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "其中，$\\mu$ 和 $\\sigma^2$ 是均值和方差，$\\gamma$ 和 $\\beta$ 是可学习的参数，$\\epsilon$ 是防止除零的小常数。\n",
    "\n",
    "#### **前馈全连接层（Feed-Forward Network, FFN）**\n",
    "前馈全连接层对每个位置的表示进行非线性变换。\n",
    "\n",
    "- **输入**：多头注意力层的输出。\n",
    "- **输出**：经过非线性变换的表示。\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "其中，$W_1, W_2$ 是权重矩阵，$b_1, b_2$ 是偏置项。\n",
    "\n",
    "#### **Softmax 和输出层**\n",
    "Softmax 用于将模型的输出转换为概率分布。\n",
    "\n",
    "- **输入**：解码器的最终输出。\n",
    "- **输出**：目标词汇的概率分布。\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^V e^{z_j}}\n",
    "$$\n",
    "其中，$z_i$ 是第 $i$ 个词汇的得分，$V$ 是词汇表大小。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evn_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
